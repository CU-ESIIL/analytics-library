{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"how-to-contribute/","title":"How to Contribute","text":"<p>For guidelines on contributing to this project, please visit our dedicated contribution page:</p> <p>https://cu-esiil.github.io/how_to_contribute/</p>","tags":["project","contributing"]},{"location":"innovation-summit-2025/","title":"Innovation Summit 2025","text":"<p>Visit the Innovation Summit website</p> <ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"style-guide/","title":"Analytics Library Style Guide","text":"<p>This document outlines conventions for adding new analyses to the library.</p>"},{"location":"style-guide/#file-naming","title":"File naming","text":"<ul> <li>Place each analysis under an appropriate subdirectory in <code>docs/</code>.</li> <li>Use short, descriptive, lowercase file names separated by hyphens (e.g. <code>my-analysis.md</code>).</li> </ul>"},{"location":"style-guide/#front-matter","title":"Front matter","text":"<p>Start every file with YAML front matter providing metadata used by the site:</p> <pre><code>---\ntitle: Descriptive title\nauthors:\n  - Your Name\ndate: YYYY-MM-DD\ntags:\n  - topic\n  - method\n---\n</code></pre>"},{"location":"style-guide/#required-sections","title":"Required sections","text":"<p>Organize each entry with the following sections:</p> <ol> <li>Description \u2013 Explain the analysis, why it is valuable, the type of data it accepts, and suggest sources from the data library.</li> <li>Usage Example \u2013 Include a self-contained, copy\u2011and\u2011pasteable code snippet that loads a dataset from the data library, performs the analysis, and produces a plot demonstrating the results.</li> <li>Interpretation and Heuristics \u2013 Provide context for reading the results and guidance on common pitfalls or rules of thumb.</li> </ol>"},{"location":"style-guide/#writing-tips","title":"Writing tips","text":"<ul> <li>Use <code>#</code> for the document title and <code>##</code> for section headings.</li> <li>Prefer concise sentences and active voice.</li> <li>Include links to relevant data library entries and external resources when helpful.</li> <li>Provide meaningful alt text for all images and plots.</li> <li>Keep code blocks executable as written; import all required libraries in the snippet.</li> </ul> <p>Adhering to these guidelines ensures consistent, discoverable, and easy-to-understand analytics examples.</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#climate","title":"climate","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> </ul>"},{"location":"tags/#cloud-correction","title":"cloud-correction","text":"<ul> <li>sentinel 2 cloud correction</li> </ul>"},{"location":"tags/#fire","title":"fire","text":"<ul> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#forecasting","title":"forecasting","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> </ul>"},{"location":"tags/#innovation-summit-2025","title":"innovation-summit-2025","text":"<p>Visit the Innovation Summit website</p> <ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#machine-learning","title":"machine-learning","text":"<ul> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#remote-sensing","title":"remote-sensing","text":"<ul> <li>sentinel 2 cloud correction</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#sentinel-2","title":"sentinel-2","text":"<ul> <li>sentinel 2 cloud correction</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#time-series","title":"time-series","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"tags/#tipping-points","title":"tipping-points","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"},{"location":"remote_sensing/post_fire_tipping_points_random_forest/","title":"Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse","text":"","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#introduction","title":"Introduction","text":"<p>Wildfires can push ecosystems across tipping points\u2014for example, from forests to shrublands\u2014when vegetation fails to recover after disturbance. This analytics entry describes a reproducible workflow that:</p> <ul> <li>Aligns fire events (from the FIRED database) with satellite vegetation time series (Sentinel\u20112 NDVI);</li> <li>Extracts early\u2011warning signals (EWS) from those time series; and</li> <li>Trains a Random Forest (RF) classifier to flag pixels that show persistent post\u2011fire vegetation collapse.</li> </ul> <p>The approach integrates three research strands: fire event mapping (FIRED), critical\u2011transition theory (EWS), and remote\u2011sensing machine learning.</p>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#data-sources","title":"Data Sources","text":"<p>Fire Events: FIRED (Fire Events Data)</p> <ul> <li>Source: Andela et al. (2019, Nature Ecology &amp; Evolution); Balch et al. (2020, CU Scholar dataset).</li> <li>Content: Event\u2011level and daily polygons delineating individual wildfires globally.</li> <li>Usage here: Event polygons for CONUS + Alaska, including ignition date (<code>ig_date</code>) and fire perimeter geometry.</li> <li>Format: GeoPackage or Shapefile; read into a GeoDataFrame (EPSG:4326).</li> </ul> <p>Vegetation Time Series: Sentinel\u20112 Level\u20112A NDVI</p> <ul> <li>Coverage: Mid\u20112015 to present.</li> <li>Resolution: 10 m (red and near\u2011infrared bands).</li> <li>Access: SpatioTemporal Asset Catalog (STAC) APIs (e.g., Earth\u2011Search by Element84).</li> <li>Representation in code: 3\u2011D array <code>(time, height, width)</code> containing NDVI values.</li> </ul> <p>Optional: Other indices (e.g., Enhanced Vegetation Index, Normalized Burn Ratio) and other sensors (e.g., Landsat, PlanetScope) can be incorporated similarly.</p>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Ecological resilience: Provides evidence on whether ecosystems exhibit critical transitions after fire.</li> <li>Management &amp; restoration: Identifies high\u2011risk areas where recovery is unlikely without intervention.</li> <li>Theory into practice: Tests whether statistical early\u2011warning indicators (e.g., variance, autocorrelation, trend) can predict persistent vegetation collapse.</li> </ul>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#workflow-stepbystep","title":"Workflow (Step\u2011by\u2011Step)","text":"<ol> <li>Select fires from FIRED (e.g., <code>ig_year &gt;= 2018</code>, <code>tot_ar_km2 &gt; 10</code>).</li> <li>Build time windows around ignition date: <code>pre_days</code> (e.g., 120) before ignition, <code>post_days</code> (e.g., 180) after ignition, split into rolling windows (<code>window_days</code>, <code>step_days</code>).</li> <li>Fetch NDVI stacks from Sentinel\u20112 via STAC for each window and the fire\u2019s bounding box, with a cloud cover filter (<code>cloud_lt</code>).</li> <li>Compute pre vs post means of NDVI for each pixel.</li> <li>Rasterize the fire polygon to the NDVI grid to identify burned pixels.</li> <li> <p>Label pixels: collapse = 1 if NDVI drop exceeds a threshold (relative or absolute); non\u2011collapse = 0 otherwise.</p> </li> <li> <p>Relative drop:      $\\text{rel_drop} = \\frac{\\text{pre_mean} - \\text{post_mean}}{|\\text{pre_mean}| + \\varepsilon}$      If <code>rel_drop &gt; \\theta</code> (e.g., (\\theta = 0.30)) inside the fire scar, label = 1.</p> </li> <li>Absolute drop: If post \u2013 pre &lt; threshold (e.g., \u20130.20), label = 1.</li> <li> <p>Extract early\u2011warning features from each pixel\u2019s NDVI time series:</p> </li> <li> <p>Slope: overall trend;</p> </li> <li>Variance: volatility over time;</li> <li>Variance ratio: short vs long window variance;</li> <li>Autocorrelation (AC1): lag\u20111 memory;</li> <li>Last, \u0394(last\u2013first), minimum, maximum: trajectory summary.</li> <li>Train a Random Forest classifier (Breiman 2001) with grouped cross\u2011validation by fire (GroupKFold) to avoid leakage across pixels from the same event.</li> <li>Evaluate performance using ROC\u2011AUC, average precision (PR\u2011AUC), precision, recall, and F1 for the positive (collapse) class.</li> <li>Interpret outputs: feature importances, per\u2011fire summaries, and optional spatial maps of predicted collapse.</li> </ol>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#mathematical-framing","title":"Mathematical Framing","text":"<p>We treat NDVI as a proxy for green biomass and canopy condition. Persistent collapse is operationalized as a significant NDVI drop post\u2011fire that does not rebound within the defined window.</p> <ul> <li>Label rule: collapse if relative or absolute NDVI decline exceeds a threshold.</li> <li>EWS features capture time\u2011series properties theorized to increase near critical transitions (Scheffer et al. 2009; Dakos et al. 2012).</li> <li>Model: Random Forest learns nonlinear mappings from feature space \u2192 probability of collapse.</li> </ul>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#model-validation","title":"Model &amp; Validation","text":"<ul> <li>Classifier: Random Forest with \\~500 trees, class\u2011weighted, <code>min_samples_leaf = 2</code>.</li> <li>Grouping: GroupKFold by fire to prevent over\u2011optimistic results.</li> <li>Metrics: ROC\u2011AUC, PR\u2011AUC, positive\u2011class precision, recall, and F1.</li> <li>Outputs: bar plot of feature importances; summary tables of per\u2011fire performance and data coverage.</li> </ul>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#example-use-case","title":"Example Use Case","text":"<p>Research question: Within large fires from 2018\u20132023, where did vegetation most likely transition to a persistent low\u2011NDVI state? Steps:</p> <ol> <li>Filter FIRED events based on size and date.</li> <li>Build dataset with <code>rel_drop_threshold=0.30</code>, <code>cloud_lt=20</code> (relax to 60 if imagery is sparse).</li> <li>Train Random Forest with grouped cross\u2011validation.</li> <li>Inspect performance and feature importance; map collapse risk within each burn.</li> <li>Export summaries to guide restoration prioritization.</li> </ol>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#limitations-best-practices","title":"Limitations &amp; Best Practices","text":"<ul> <li>Label uncertainty: NDVI collapse does not guarantee irreversible ecological state change; field data are crucial for validation.</li> <li>Window design: At least 2\u20133 windows pre\u2011 and post\u2011fire are needed to avoid mistaking short\u2011term scorch for collapse.</li> <li>Cloud/seasonal effects: Clouds and seasonal phenology can bias NDVI features; seasonal adjustment is recommended.</li> <li>Domain shift: Models trained on one ecoregion or period may not generalize elsewhere; always use grouped CV and out\u2011of\u2011region testing.</li> </ul>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#references","title":"References","text":"<ul> <li>Andela, N., Morton, D. C., Giglio, L., Chen, Y., van der Werf, G. R., Kasibhatla, P. S., DeFries, R. S., Collatz, G. J., Hantson, S., Kloster, S., Bachelet, D., Forrest, M., Lasslop, G., Li, F., Mangeon, S., Melton, J. R., Yue, C., &amp; Randerson, J. T. (2019). The Global Fire Atlas of individual fire size, duration, speed and direction. Nature Ecology &amp; Evolution, 3, 1494\u20131502. https://doi.org/10.1038/s41559-019-0386-1</li> <li>Balch, J. K., et al. (2020). FIRED: a global fire event database for the analysis of fire regimes, patterns, and drivers. CU Scholar Dataset.</li> <li>Scheffer, M., Bascompte, J., Brock, W. A., Brovkin, V., Carpenter, S. R., Dakos, V., Held, H., van Nes, E. H., Rietkerk, M., &amp; Sugihara, G. (2009). Early\u2011warning signals for critical transitions. Nature, 461, 53\u201359. https://doi.org/10.1038/nature08227</li> <li>Dakos, V., Carpenter, S. R., Brock, W. A., Ellison, A. M., Guttal, V., Ives, A. R., K\u00e9fi, S., Livina, V., Seekell, D. A., van Nes, E. H., &amp; Scheffer, M. (2012). Methods for detecting early warnings of critical transitions in time series: A review. PLOS ONE, 7(7), e41010. https://doi.org/10.1371/journal.pone.0041010</li> <li>K\u00e9fi, S., Guttal, V., Brock, W. A., Carpenter, S. R., Ellison, A. M., Livina, V. N., Seekell, D. A., van Nes, E. H., &amp; Scheffer, M. (2014). Early warning signals of ecological transitions: methods for spatial patterns. Philosophical Transactions of the Royal Society B, 370(1659), 20130283. https://doi.org/10.1098/rstb.2013.0283</li> <li>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5\u201332. https://doi.org/10.1023/A:1010933404324</li> <li>Hesketh, M., et al. (2021). Detecting post\u2011fire vegetation recovery using remote sensing and machine learning. Remote Sensing of Environment, 257, 112210. https://doi.org/10.1016/j.rse.2020.112210</li> <li>Liu, X., et al. (2020). Remote sensing\u2011based early warning of vegetation degradation. Ecological Indicators, 115, 106764. https://doi.org/10.1016/j.ecolind.2020.106764</li> </ul>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/post_fire_tipping_points_random_forest/#code","title":"Code","text":"<p>Note: Requires rasterio, numpy, pandas, shapely, pystac-client, scikit-learn, and matplotlib.</p> <pre><code>\n# ===================== DEPENDENCIES: fetch_ndvi_stack + _extract_ews_features =====================\n# Requires: rasterio, numpy, pandas, pystac-client (pip install pystac-client)\n# Notes:\n#  - Works with Sentinel-2 L2A on Element84 Earth Search or compatible STAC.\n#  - Uses per-window \"best single item\" by lowest eo:cloud_cover.\n#  - Warps all reads to a common EPSG:4326 grid aligned to your bbox.\n\nfrom types import SimpleNamespace\nfrom pystac_client import Client\nimport rasterio\nfrom rasterio.vrt import WarpedVRT\nfrom rasterio.warp import Resampling\nfrom rasterio.transform import from_bounds\n\ndef fetch_ndvi_stack(\n    stac_api: str,\n    collection: str,\n    date_ranges: Sequence[str],   # list like [\"YYYY-MM-DDT00:00:00Z/YYYY-MM-DDT23:59:59Z\", ...]\n    bbox: Sequence[float],        # [minx, miny, maxx, maxy] in EPSG:4326\n    cloud_lt: int = 20,\n    overview_level: int = 4,      # ignored in this simplified version; grid size set by bbox\n    max_search_items: int = 50,\n    prefer_smallest_cloud: bool = True,\n) -&gt; SimpleNamespace:\n    \"\"\"\n    Returns: SimpleNamespace(\n        ndvi = np.ndarray shape (T,H,W) with float32 in [-1,1] (NaN where missing),\n        dates = list of ISO strings (len T),\n        transform = affine transform for EPSG:4326 grid,\n        crs = 'EPSG:4326',\n        bbox = tuple(minx,miny,maxx,maxy)\n    )\n    \"\"\"\n    # 1) Choose a reasonable grid for the bbox (aim ~512 px on the long side)\n    minx, miny, maxx, maxy = map(float, bbox)\n    dx, dy = maxx - minx, maxy - miny\n    long_side = max(dx, dy)\n    target_long_pixels = 512\n    # keep aspect ratio\n    if dx &gt;= dy:\n        W = target_long_pixels\n        H = max(1, int(round(target_long_pixels * (dy / dx)))) if dx &gt; 0 else 1\n    else:\n        H = target_long_pixels\n        W = max(1, int(round(target_long_pixels * (dx / dy)))) if dy &gt; 0 else 1\n    transform = from_bounds(minx, miny, maxx, maxy, W, H)\n    crs = \"EPSG:4326\"\n\n    # 2) Open STAC\n    client = Client.open(stac_api)\n\n    ndvi_slices = []\n    out_dates = []\n\n    # Helper to pick item with lowest cloud cover\n    def _pick_item(items):\n        if not items:\n            return None\n        if prefer_smallest_cloud:\n            items = sorted(items, key=lambda it: it.properties.get(\"eo:cloud_cover\", 1000))\n        return items[0]\n\n    # 3) For each date window, search, pick best item, compute NDVI onto our grid\n    for dr in date_ranges:\n        search = client.search(\n            collections=[collection],\n            bbox=[minx, miny, maxx, maxy],\n            datetime=dr,\n            query={\"eo:cloud_cover\": {\"lt\": cloud_lt}},\n            limit=max_search_items,\n        )\n        items = list(search.get_items())\n        item = _pick_item(items)\n        if item is None:\n            # No imagery for this window -&gt; append NaN slice\n            ndvi_slices.append(np.full((H, W), np.nan, dtype=\"float32\"))\n            out_dates.append(dr.split(\"/\")[1][:10] if \"/\" in dr else dr[:10])\n            continue\n\n        # Sentinel-2 L2A common keys for red &amp; nir; fallback names added just in case\n        # (Element84 uses e.g., 'red' and 'nir' asset keys)\n        red_asset = item.assets.get(\"red\") or item.assets.get(\"B04\") or item.assets.get(\"B4\")\n        nir_asset = item.assets.get(\"nir\") or item.assets.get(\"B08\") or item.assets.get(\"B8\")\n        if (red_asset is None) or (nir_asset is None):\n            ndvi_slices.append(np.full((H, W), np.nan, dtype=\"float32\"))\n            out_dates.append(item.properties.get(\"datetime\", dr)[:10])\n            continue\n\n        # Read and warp to our target grid in EPSG:4326\n        def _read_warped(url: str) -&gt; np.ndarray:\n            with rasterio.open(url) as src:\n                vrt_opts = dict(crs=crs, transform=transform, width=W, height=H, resampling=Resampling.bilinear)\n                with WarpedVRT(src, **vrt_opts) as vrt:\n                    arr = vrt.read(1, out_dtype=\"float32\", masked=True)\n                    return np.where(arr.mask, np.nan, arr.filled(np.nan)).astype(\"float32\")\n\n        try:\n            red = _read_warped(red_asset.href)\n            nir = _read_warped(nir_asset.href)\n            ndvi = (nir - red) / (nir + red + 1e-6)\n            # Clip to plausible range\n            ndvi = np.clip(ndvi, -1.0, 1.0).astype(\"float32\")\n        except Exception:\n            ndvi = np.full((H, W), np.nan, dtype=\"float32\")\n\n        ndvi_slices.append(ndvi)\n        out_dates.append(item.properties.get(\"datetime\", dr)[:10])\n\n    if not ndvi_slices:\n        raise RuntimeError(\"fetch_ndvi_stack: no NDVI slices were created.\")\n\n    ndvi_stack = np.stack(ndvi_slices, axis=0)  # (T,H,W)\n    return SimpleNamespace(ndvi=ndvi_stack, dates=out_dates, transform=transform, crs=crs, bbox=(minx, miny, maxx, maxy))\n\n# -------------------- EWS feature extraction from (T,H,W) NDVI cube --------------------\ndef _extract_ews_features(ndvi_stack: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Convert (T,H,W) NDVI into per-pixel features expected by your model.\n    Features (all HxW):\n      - slope: least-squares trend over t=0..T-1\n      - var_all: variance over full series\n      - var_ratio: variance(second half) / (variance(first half)+eps)\n      - ac1: lag-1 autocorrelation\n      - last: last finite value\n      - d_last_first: last - first finite\n      - vmin, vmax: min/max over series\n    \"\"\"\n    if ndvi_stack.ndim != 3:\n        raise ValueError(f\"_extract_ews_features expects (T,H,W), got {ndvi_stack.shape}\")\n\n    T, H, W = ndvi_stack.shape\n    x = np.arange(T, dtype=\"float32\")[:, None, None]\n    y = ndvi_stack.astype(\"float32\")\n\n    # Masks\n    valid = np.isfinite(y)\n\n    # --- slope via least squares on demeaned x ---\n    x0 = x - x.mean(axis=0, keepdims=True)  # center time\n    y0 = np.where(valid, y, 0.0)\n    denom = (x0**2).sum(axis=0) + 1e-6\n    slope = ((x0 * y0).sum(axis=0)) / denom\n\n    # --- variance over all, and halves ---\n    with np.errstate(invalid=\"ignore\"):\n        var_all = np.nanvar(y, axis=0)\n        mid = T // 2\n        var_first = np.nanvar(y[:mid], axis=0) if mid &gt; 1 else np.full((H, W), np.nan, dtype=\"float32\")\n        var_second = np.nanvar(y[mid:], axis=0) if (T - mid) &gt; 1 else np.full((H, W), np.nan, dtype=\"float32\")\n        var_ratio = var_second / (np.abs(var_first) + 1e-6)\n\n    # --- lag-1 autocorrelation ---\n    def _nanmean(a, axis=0):\n        with np.errstate(invalid=\"ignore\"):\n            return np.nanmean(a, axis=axis)\n    mu = _nanmean(y, axis=0)\n    y_center = y - mu\n    y_lag = y_center[1:]\n    y_lead = y_center[:-1]\n    num = (y_lead * y_lag)\n    den = (y_lead**2)\n    # sum over time while ignoring NaNs\n    num_sum = np.nansum(num, axis=0)\n    den_sum = np.nansum(den, axis=0) + 1e-6\n    ac1 = num_sum / den_sum\n\n    # --- last, first, delta ---\n    def _nan_first(a):\n        # first finite along axis=0\n        idx = np.argmax(np.isfinite(a), axis=0)\n        out = a[idx, np.arange(a.shape[1])[:, None], np.arange(a.shape[2])]\n        out[~np.isfinite(out)] = np.nan\n        return out\n    def _nan_last(a):\n        # last finite along axis=0\n        rev = np.flip(a, axis=0)\n        idx = np.argmax(np.isfinite(rev), axis=0)\n        out = rev[idx, np.arange(a.shape[1])[:, None], np.arange(a.shape[2])]\n        out[~np.isfinite(out)] = np.nan\n        return out\n\n    first = _nan_first(y)\n    last = _nan_last(y)\n    d_last_first = last - first\n\n    # --- min/max over time ---\n    vmin = np.nanmin(y, axis=0)\n    vmax = np.nanmax(y, axis=0)\n\n    # sanitize dtypes\n    def f32(a): return a.astype(\"float32\", copy=False)\n\n    return dict(\n        slope=f32(slope),\n        var_all=f32(var_all),\n        var_ratio=f32(var_ratio),\n        ac1=f32(ac1),\n        last=f32(last),\n        d_last_first=f32(d_last_first),\n        vmin=f32(vmin),\n        vmax=f32(vmax),\n    )\n    # ===================== MULTI-FIRE RF TIPPING (3-FUNCTION SCRIPT) =====================\n# Public API:\n#   1) get_data_and_labels(fired_gdf, ...)\n#   2) run_model(data, ...)\n#   3) produce_figures(fired_rows, rf, feature_names, ...)\n#\n# Requires: numpy, pandas, shapely, rasterio, scikit-learn, matplotlib\n# Expects you ALREADY HAVE:\n#   - fetch_ndvi_stack(stac_api, collection, date_ranges, bbox, cloud_lt, overview_level)\n#   - _extract_ews_features(ndvi_stack) -&gt; dict with keys:\n#       [\"slope\", \"var_all\", \"var_ratio\", \"ac1\", \"last\", \"d_last_first\", \"vmin\", \"vmax\"]\n\nimport sys\nimport warnings\nimport datetime as dt\nfrom typing import Sequence, Dict, Any, Optional, Tuple, List, Iterable\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom shapely.geometry import mapping\nimport rasterio\nfrom rasterio import features as rfeatures\nfrom rasterio.transform import from_bounds\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n\n# ------------------------------ small helpers ------------------------------\ndef _make_rolling_windows(start: dt.datetime, end: dt.datetime,\n                          step_days: int = 12, window_days: int = 10) -&gt; List[str]:\n    if end &lt; start:\n        return []\n    out = []\n    cur = start\n    while cur &lt;= end:\n        w_end = min(end, cur + dt.timedelta(days=window_days - 1))\n        out.append(f\"{cur.strftime('%Y-%m-%d')}T00:00:00Z/{w_end.strftime('%Y-%m-%d')}T23:59:59Z\")\n        cur += dt.timedelta(days=step_days)\n    return out\n\ndef _bbox_from_geom(geom, pad_frac: float = 0.02) -&gt; Tuple[float, float, float, float]:\n    minx, miny, maxx, maxy = geom.bounds\n    dx, dy = maxx - minx, maxy - miny\n    pad_x, pad_y = dx * pad_frac, dy * pad_frac\n    return (minx - pad_x, miny - pad_y, maxx + pad_x, maxy + pad_y)\n\ndef _coerce_ts(val) -&gt; pd.Timestamp:\n    if isinstance(val, (pd.Timestamp, np.datetime64)):\n        return pd.Timestamp(val)\n    if isinstance(val, (dt.datetime, dt.date)):\n        return pd.Timestamp(val)\n    return pd.to_datetime(val, errors=\"raise\", utc=False)\n\ndef _fire_ig_date(row) -&gt; dt.datetime:\n    \"\"\"\n    Robustly extract ignition date from a FIRED row (from .itertuples()).\n    Tries common variants and falls back to scanning attribute names.\n    \"\"\"\n    likely_names = [\n        \"ig_date\", \"ignition_date\", \"start_date\", \"ig_dt\", \"start_dt\",\n        \"date_ignition\", \"igdate\", \"DATE_IGNITION\", \"IG_DATE\"\n    ]\n    for nm in likely_names:\n        if hasattr(row, nm):\n            return _coerce_ts(getattr(row, nm)).to_pydatetime()\n\n    for nm in getattr(row, \"_fields\", []):\n        if (\"ig\" in nm.lower() or \"ignit\" in nm.lower() or \"start\" in nm.lower()) and \"date\" in nm.lower():\n            return _coerce_ts(getattr(row, nm)).to_pydatetime()\n\n    for nm in getattr(row, \"_fields\", []):\n        val = getattr(row, nm)\n        try:\n            ts = _coerce_ts(val)\n            if 2000 &lt;= ts.year &lt;= 2035:\n                return ts.to_pydatetime()\n        except Exception:\n            pass\n\n    raise ValueError(\"Could not find an ignition date column on this row (e.g., 'ig_date').\")\n\ndef _print_once(msg: str, key: str, seen: set):\n    if key not in seen:\n        print(msg, file=sys.stderr)\n        seen.add(key)\n\n# ------------------------------ core builder ------------------------------\ndef _build_dataset_from_fires(\n    fired_gdf,                              # GeoDataFrame with FIRED events (EPSG:4326)\n    stac_api: str = \"https://earth-search.aws.element84.com/v1\",\n    collection: str = \"sentinel-2-l2a\",\n    select_idx: Optional[Sequence[int]] = None,  # row indices to use (or None for all)\n    pre_days: int = 120,\n    post_days: int = 180,\n    step_days: int = 12,\n    window_days: int = 10,\n    cloud_lt: int = 20,\n    overview_level: int = 4,\n    # labeling options\n    use_relative_drop: bool = True,\n    abs_drop_threshold: float = -0.20,\n    rel_drop_threshold: float = 0.30,\n    # sampling / memory\n    max_pixels_per_fire: int = 60000,\n    random_state: int = 42,\n    verbose: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Loops over many FIRED polygons, builds features+labels per fire, and concatenates\n    into a single dataset with group IDs for CV.\n    Returns: dict with X, y, groups, feature_names, per_fire_records, failures, logs.\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    iterator = (fired_gdf.itertuples(index=True) if select_idx is None\n                else fired_gdf.iloc[list(select_idx)].itertuples(index=True))\n\n    all_X, all_y, all_groups = [], [], []\n    per_fire_records, failures, logs = [], [], []\n    feature_names = [\"slope\", \"var_all\", \"var_ratio\", \"ac1\", \"last\", \"d_last_first\", \"vmin\", \"vmax\"]\n    fire_counter = 0\n    seen_msgs = set()\n\n    for row in iterator:\n        idx = row.Index\n        try:\n            ig = _fire_ig_date(row)\n            geom = row.geometry\n            if geom is None or geom.is_empty:\n                raise ValueError(\"Empty geometry.\")\n\n            pre_start  = ig - dt.timedelta(days=pre_days)\n            pre_end    = ig - dt.timedelta(days=1)\n            post_start = ig + dt.timedelta(days=1)\n            post_end   = ig + dt.timedelta(days=post_days)\n\n            pre_windows  = _make_rolling_windows(pre_start,  pre_end,  step_days, window_days)\n            post_windows = _make_rolling_windows(post_start, post_end, step_days, window_days)\n            if len(pre_windows) &lt; 2 or len(post_windows) &lt; 2:\n                raise ValueError(f\"Too few time windows (pre={len(pre_windows)}, post={len(post_windows)}).\")\n            date_ranges = pre_windows + post_windows\n\n            bbox = _bbox_from_geom(geom, pad_frac=0.02)\n\n            # ---- Your external helper: fetch_ndvi_stack ----\n            stack = fetch_ndvi_stack(\n                stac_api=stac_api,\n                collection=collection,\n                date_ranges=date_ranges,\n                bbox=list(bbox),\n                cloud_lt=cloud_lt,\n                overview_level=overview_level,\n            )\n            if not hasattr(stack, \"ndvi\"):\n                raise ValueError(\"fetch_ndvi_stack returned object without .ndvi\")\n\n            arr = np.asarray(stack.ndvi)  # (T,H,W)\n            if arr.ndim != 3:\n                raise ValueError(f\"NDVI stack shape {arr.shape} (expected T,H,W).\")\n            T, H, W = arr.shape\n            if T &lt; 4 or H &lt; 8 or W &lt; 8:\n                raise ValueError(f\"NDVI stack too small or empty: {arr.shape}\")\n\n            mid = len(date_ranges) // 2\n            if mid &lt;= 0 or mid &gt;= T:\n                raise ValueError(f\"Split index invalid (mid={mid}, T={T}).\")\n\n            pre_mean  = np.nanmean(arr[:mid], axis=0)\n            post_mean = np.nanmean(arr[mid:], axis=0)\n            ndvi_delta = post_mean - pre_mean\n\n            transform = rasterio.transform.from_bounds(*bbox, W, H)\n            fire_mask = rfeatures.rasterize(\n                [(mapping(geom), 1)], out_shape=(H, W), transform=transform, fill=0, dtype=np.uint8\n            )\n\n            if use_relative_drop:\n                rel = (pre_mean - post_mean) / (np.abs(pre_mean) + 1e-6)\n                labels = ((fire_mask == 1) &amp; np.isfinite(rel) &amp; (rel &gt; rel_drop_threshold)).astype(np.uint8)\n            else:\n                labels = ((fire_mask == 1) &amp; np.isfinite(ndvi_delta) &amp; (ndvi_delta &lt; abs_drop_threshold)).astype(np.uint8)\n\n            # ---- Your external helper: _extract_ews_features ----\n            feats = _extract_ews_features(arr)\n            missing = [k for k in feature_names if k not in feats]\n            if missing:\n                raise ValueError(f\"_extract_ews_features missing keys: {missing}\")\n\n            X_full = np.stack([feats[k] for k in feature_names], axis=-1).reshape(-1, len(feature_names))\n            y_full = labels.reshape(-1)\n\n            ok = np.isfinite(X_full).all(axis=1) &amp; np.isfinite(y_full)\n            X_full, y_full = X_full[ok], y_full[ok]\n            if X_full.size == 0:\n                raise ValueError(\"No valid pixels after NaN filtering.\")\n\n            if X_full.shape[0] &gt; max_pixels_per_fire:\n                sel = rng.choice(np.arange(X_full.shape[0]), size=max_pixels_per_fire, replace=False)\n                X_full, y_full = X_full[sel], y_full[sel]\n\n            n_pos = int(y_full.sum())\n            if n_pos == 0:\n                _print_once(\"Note: some fires have 0 positives at rel_drop=0.30. \"\n                            \"Try rel_drop_threshold=0.15 or abs_drop mode.\", \"zero_pos\", seen_msgs)\n                logs.append(dict(fire_index=int(idx), message=\"Zero positives; consider relaxing thresholds.\"))\n                continue\n\n            all_X.append(X_full)\n            all_y.append(y_full)\n            all_groups.append(np.full(y_full.shape[0], fire_counter, dtype=np.int32))\n\n            per_fire_records.append(dict(\n                fire_index=int(idx),\n                ig_date=str(pd.Timestamp(ig).date()),\n                n_pixels=int(y_full.size),\n                n_positive=int(n_pos),\n                bbox=bbox\n            ))\n            fire_counter += 1\n\n        except Exception as e:\n            msg = f\"[fire idx {idx}] {type(e).__name__}: {e}\"\n            failures.append(dict(fire_index=int(idx), error=str(e)))\n            if verbose:\n                print(msg, file=sys.stderr)\n\n    if not all_X:\n        raise RuntimeError(\n            \"No fires produced training data. Common fixes:\\n\"\n            \"  \u2022 Relax thresholds: rel_drop_threshold=0.15 (or 0.10)\\n\"\n            \"  \u2022 Loosen cloud filter: cloud_lt=60\\n\"\n            \"  \u2022 Widen windows: window_days=20, step_days=10 (ensure &gt;=2 pre and &gt;=2 post)\\n\"\n            \"  \u2022 Inspect `failures` for column name / imagery issues.\"\n        )\n\n    X = np.vstack(all_X)\n    y = np.concatenate(all_y)\n    groups = np.concatenate(all_groups)\n\n    return dict(\n        X=X, y=y, groups=groups,\n        feature_names=[\"slope\", \"var_all\", \"var_ratio\", \"ac1\", \"last\", \"d_last_first\", \"vmin\", \"vmax\"],\n        per_fire_records=per_fire_records,\n        failures=failures,\n        logs=logs\n    )\n\ndef _train_rf_groupcv(\n    X: np.ndarray,\n    y: np.ndarray,\n    groups: np.ndarray,\n    feature_names: Sequence[str],\n    n_splits: int = 5,\n    random_state: int = 42,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Grouped cross-validation by fire: no pixel leakage across fires.\n    Returns per-fold metrics, final model fit on all data, and feature importances.\n    \"\"\"\n    gkf = GroupKFold(n_splits=min(n_splits, len(np.unique(groups))))\n    rf_params = dict(\n        n_estimators=500, max_depth=None, min_samples_leaf=2,\n        class_weight=\"balanced\", n_jobs=-1, random_state=random_state\n    )\n\n    fold_reports = []\n    y_true_all, y_prob_all = [], []\n\n    for k, (tr, te) in enumerate(gkf.split(X, y, groups)):\n        rf = RandomForestClassifier(**rf_params)\n        rf.fit(X[tr], y[tr])\n\n        prob = rf.predict_proba(X[te])[:, 1]\n        pred = (prob &gt;= 0.5).astype(np.uint8)\n\n        y_true_all.append(y[te]); y_prob_all.append(prob)\n\n        rpt = classification_report(y[te], pred, digits=3, output_dict=True, zero_division=0)\n        try:\n            roc = roc_auc_score(y[te], prob)\n            pr  = average_precision_score(y[te], prob)\n        except Exception:\n            roc, pr = np.nan, np.nan\n\n        fold_reports.append(dict(\n            fold=k+1,\n            support_pos=int((y[te]==1).sum()),\n            precision_pos=rpt.get(\"1\", {}).get(\"precision\", np.nan),\n            recall_pos=rpt.get(\"1\", {}).get(\"recall\", np.nan),\n            f1_pos=rpt.get(\"1\", {}).get(\"f1-score\", np.nan),\n            roc_auc=roc,\n            avg_precision=pr\n        ))\n\n    rf_final = RandomForestClassifier(**rf_params).fit(X, y)\n    importances = rf_final.feature_importances_\n\n    df_rep = pd.DataFrame(fold_reports)\n    with np.errstate(invalid='ignore'):\n        summary = dict(\n            folds=len(df_rep),\n            mean_precision_pos=float(df_rep[\"precision_pos\"].mean()),\n            mean_recall_pos=float(df_rep[\"recall_pos\"].mean()),\n            mean_f1_pos=float(df_rep[\"f1_pos\"].mean()),\n            mean_roc_auc=float(df_rep[\"roc_auc\"].mean()),\n            mean_avg_precision=float(df_rep[\"avg_precision\"].mean())\n        )\n\n    # Ready-made importances fig (optional display)\n    fig_imp = plt.figure(figsize=(5, 3.5))\n    order = np.argsort(importances)\n    plt.barh(np.array(feature_names)[order], importances[order])\n    plt.title(\"Random Forest feature importances\")\n    plt.tight_layout()\n\n    return dict(\n        rf=rf_final,\n        feature_names=list(feature_names),\n        importances=importances,\n        cv_folds=df_rep,\n        cv_summary=summary,\n        fig_importances=fig_imp\n    )\n\ndef _plot_faceted_fires(\n    fired_rows: Iterable,\n    rf: RandomForestClassifier,\n    feature_names: Sequence[str],\n    *,\n    stac_api: str = \"https://earth-search.aws.element84.com/v1\",\n    collection: str = \"sentinel-2-l2a\",\n    pre_days: int = 120,\n    post_days: int = 150,\n    step_days: int = 10,\n    window_days: int = 20,\n    cloud_lt: int = 60,\n    overview_level: int = 4,\n    pad_frac: float = 0.02,\n    ndvi_vmin: float = -0.2,\n    ndvi_vmax: float = 0.9,\n    prob_vmin: float = 0.0,\n    prob_vmax: float = 1.0,\n    max_fires: int = 6,\n    suptitle: str = \"Sentinel-2 NDVI Early-Warning + Random Forest Tipping Analysis\"\n):\n    \"\"\"\n    For each fire:\n      Col 1: Latest NDVI frame\n      Col 2: RF probability (masked to perimeter)\n      Col 3: Global feature importances (same for all rows)\n    Returns (fig, panels)\n    \"\"\"\n    rows_list = list(fired_rows)[:max_fires]\n    if not rows_list:\n        raise ValueError(\"No fires provided to _plot_faceted_fires.\")\n\n    n = len(rows_list)\n    ncols = 3\n    nrows = n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4.2*n), constrained_layout=True)\n    if n == 1:\n        axes = np.array([axes])\n\n    panels = []\n    importances = rf.feature_importances_\n    order = np.argsort(importances)\n    fnames = np.array(feature_names)\n\n    for rix, row in enumerate(rows_list):\n        idx = getattr(row, \"Index\", rix)\n        try:\n            ig = _fire_ig_date(row)\n            geom = row.geometry\n            if geom is None or geom.is_empty:\n                raise ValueError(\"Empty geometry.\")\n\n            pre_start  = ig - dt.timedelta(days=pre_days)\n            pre_end    = ig - dt.timedelta(days=1)\n            post_start = ig + dt.timedelta(days=1)\n            post_end   = ig + dt.timedelta(days=post_days)\n            pre_windows  = _make_rolling_windows(pre_start,  pre_end,  step_days, window_days)\n            post_windows = _make_rolling_windows(post_start, post_end, step_days, window_days)\n            if len(pre_windows) &lt; 2 or len(post_windows) &lt; 2:\n                raise ValueError(f\"Too few time windows (pre={len(pre_windows)}, post={len(post_windows)}).\")\n            date_ranges = pre_windows + post_windows\n\n            bbox = _bbox_from_geom(geom, pad_frac=pad_frac)\n            stack = fetch_ndvi_stack(\n                stac_api=stac_api, collection=collection,\n                date_ranges=date_ranges, bbox=list(bbox),\n                cloud_lt=cloud_lt, overview_level=overview_level\n            )\n            if not hasattr(stack, \"ndvi\"):\n                raise ValueError(\"fetch_ndvi_stack returned object without .ndvi\")\n            arr = np.asarray(stack.ndvi)  # (T,H,W)\n            if arr.ndim != 3:\n                raise ValueError(f\"NDVI stack has unexpected shape {arr.shape} (expected T,H,W).\")\n            T, H, W = arr.shape\n            transform = from_bounds(*bbox, W, H)\n\n            latest_ndvi = arr[-1]\n\n            feats = _extract_ews_features(arr)\n            missing = [k for k in feature_names if k not in feats]\n            if missing:\n                raise ValueError(f\"_extract_ews_features missing keys: {missing}\")\n            feat_stack = np.stack([feats[k] for k in feature_names], axis=-1)  # (H,W,F)\n            flat = feat_stack.reshape(-1, feat_stack.shape[-1])\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                prob = rf.predict_proba(flat)[:, 1].reshape(H, W)\n\n            fire_mask = rfeatures.rasterize(\n                [(mapping(geom), 1)],\n                out_shape=(H, W), transform=transform, fill=0, dtype=np.uint8\n            ).astype(bool)\n            prob_masked = np.where(fire_mask, prob, np.nan)\n\n            # --- Col 1: NDVI ---\n            ax1 = axes[rix, 0]\n            im1 = ax1.imshow(\n                latest_ndvi, origin=\"upper\",\n                extent=[bbox[0], bbox[2], bbox[1], bbox[3]],\n                vmin=ndvi_vmin, vmax=ndvi_vmax, cmap=\"viridis\", interpolation=\"nearest\"\n            )\n            gtype = getattr(geom, \"geom_type\", None) or getattr(geom, \"type\", None)\n            if gtype in (\"Polygon\", \"MultiPolygon\"):\n                polys = [geom] if gtype == \"Polygon\" else list(geom.geoms)\n                for poly in polys:\n                    x, y = poly.exterior.xy\n                    ax1.plot(x, y, lw=1.0, color=\"white\")\n            ax1.set_ylabel(\"Latitude\")\n            ax1.set_title(f\"Latest NDVI \u2022 {pd.Timestamp(date_ranges[-1].split('/')[1][:10]).date()}\")\n            cbar1 = fig.colorbar(im1, ax=ax1, fraction=0.035, pad=0.02); cbar1.set_label(\"NDVI\")\n\n            # --- Col 2: RF probability ---\n            ax2 = axes[rix, 1]\n            im2 = ax2.imshow(\n                prob_masked, origin=\"upper\",\n                extent=[bbox[0], bbox[2], bbox[1], bbox[3]],\n                vmin=prob_vmin, vmax=prob_vmax, cmap=\"viridis\", interpolation=\"nearest\"\n            )\n            for poly in polys:\n                x, y = poly.exterior.xy\n                ax2.plot(x, y, lw=1.0, color=\"black\")\n            ax2.set_xlabel(\"Longitude\")\n            ax2.set_title(\"Random Forest Tipping-Risk (probability)\")\n            cbar2 = fig.colorbar(im2, ax=ax2, fraction=0.035, pad=0.02); cbar2.set_label(\"RF probability (tipping)\")\n\n            # --- Col 3: Feature importances (global) ---\n            ax3 = axes[rix, 2]\n            ax3.barh(fnames[order], importances[order])\n            ax3.set_title(\"Random Forest Feature Importances\")\n            ax3.set_xlim(0, max(0.001, float(importances.max()) * 1.15))\n            for spine in (\"top\", \"right\"):\n                ax3.spines[spine].set_visible(False)\n\n            panels.append(dict(\n                fire_index=idx, bbox=bbox, ndvi=latest_ndvi, prob=prob_masked,\n                transform=transform, geometry=geom\n            ))\n\n        except Exception as e:\n            for c in range(ncols):\n                axes[rix, c].axis(\"off\")\n            axes[rix, 0].text(0.02, 0.5, f\"Fire {idx}: {type(e).__name__}\\n{e}\",\n                              transform=axes[rix, 0].transAxes, ha=\"left\", va=\"center\")\n            panels.append(dict(fire_index=idx, error=str(e)))\n\n    fig.suptitle(suptitle, y=0.995, fontsize=14)\n    return fig, panels\n\n# =============================== PUBLIC API (3 functions) ===============================\ndef get_data_and_labels(\n    fired_gdf,\n    *,\n    stac_api: str = \"https://earth-search.aws.element84.com/v1\",\n    collection: str = \"sentinel-2-l2a\",\n    select_idx: Optional[Sequence[int]] = None,\n    pre_days: int = 120,\n    post_days: int = 180,\n    step_days: int = 12,\n    window_days: int = 10,\n    cloud_lt: int = 20,\n    overview_level: int = 4,\n    use_relative_drop: bool = True,\n    abs_drop_threshold: float = -0.20,\n    rel_drop_threshold: float = 0.30,\n    max_pixels_per_fire: int = 60000,\n    random_state: int = 42,\n    verbose: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Wrapper that builds the dataset (X, y, groups, metadata).\"\"\"\n    return _build_dataset_from_fires(\n        fired_gdf=fired_gdf,\n        stac_api=stac_api,\n        collection=collection,\n        select_idx=select_idx,\n        pre_days=pre_days,\n        post_days=post_days,\n        step_days=step_days,\n        window_days=window_days,\n        cloud_lt=cloud_lt,\n        overview_level=overview_level,\n        use_relative_drop=use_relative_drop,\n        abs_drop_threshold=abs_drop_threshold,\n        rel_drop_threshold=rel_drop_threshold,\n        max_pixels_per_fire=max_pixels_per_fire,\n        random_state=random_state,\n        verbose=verbose,\n    )\n\ndef run_model(\n    data: Dict[str, Any],\n    *,\n    n_splits: int = 5,\n    random_state: int = 42,\n) -&gt; Dict[str, Any]:\n    \"\"\"Wrapper that trains the Random Forest with grouped CV and returns results.\"\"\"\n    return _train_rf_groupcv(\n        X=data[\"X\"],\n        y=data[\"y\"],\n        groups=data[\"groups\"],\n        feature_names=data[\"feature_names\"],\n        n_splits=n_splits,\n        random_state=random_state,\n    )\n\ndef produce_figures(\n    fired_rows: Iterable,\n    rf: RandomForestClassifier,\n    feature_names: Sequence[str],\n    *,\n    stac_api: str = \"https://earth-search.aws.element84.com/v1\",\n    collection: str = \"sentinel-2-l2a\",\n    pre_days: int = 120,\n    post_days: int = 150,\n    step_days: int = 10,\n    window_days: int = 20,\n    cloud_lt: int = 60,\n    overview_level: int = 4,\n    pad_frac: float = 0.02,\n    ndvi_vmin: float = -0.2,\n    ndvi_vmax: float = 0.9,\n    prob_vmin: float = 0.0,\n    prob_vmax: float = 1.0,\n    max_fires: int = 6,\n    suptitle: str = \"Sentinel-2 NDVI Early-Warning + Random Forest Tipping Analysis\"\n):\n    \"\"\"Wrapper that produces the faceted per-fire panels (figure + panels list).\"\"\"\n    return _plot_faceted_fires(\n        fired_rows=fired_rows,\n        rf=rf,\n        feature_names=feature_names,\n        stac_api=stac_api,\n        collection=collection,\n        pre_days=pre_days,\n        post_days=post_days,\n        step_days=step_days,\n        window_days=window_days,\n        cloud_lt=cloud_lt,\n        overview_level=overview_level,\n        pad_frac=pad_frac,\n        ndvi_vmin=ndvi_vmin,\n        ndvi_vmax=ndvi_vmax,\n        prob_vmin=prob_vmin,\n        prob_vmax=prob_vmax,\n        max_fires=max_fires,\n        suptitle=suptitle\n    )\n\n# =============================== EXAMPLE (comment out if importing) ===============================\n# Example usage in a notebook (assumes you have `fired_events` GeoDataFrame available):\n# fires_sel = fired_events.query(\"ig_year &gt;= 2018 and tot_ar_km2 &gt; 10\").head(5)\n# data = get_data_and_labels(\n#     fired_gdf=fires_sel,\n#     pre_days=120, post_days=150,\n#     step_days=10, window_days=20,\n#     cloud_lt=60,\n#     use_relative_drop=True, rel_drop_threshold=0.15,\n#     max_pixels_per_fire=40000,\n#     verbose=True\n# )\n# print(\"Per-fire head:\\n\", pd.DataFrame(data[\"per_fire_records\"]).head())\n# print(\"Failures:\", len(data[\"failures\"]))\n# res = run_model(data, n_splits=5)\n# print(\"CV summary:\", res[\"cv_summary\"])\n# display(res[\"fig_importances\"])\n#\n# fig_facets, panels = produce_figures(\n#     fired_rows=fires_sel.head(3).itertuples(index=True),\n#    rf=res[\"rf\"],\n#     feature_names=data[\"feature_names\"],\n#     stac_api=\"https://earth-search.aws.element84.com/v1\",\n#     collection=\"sentinel-2-l2a\",\n#     pre_days=120, post_days=150,\n#     step_days=10, window_days=20,\n#     cloud_lt=60,\n#     overview_level=4,\n#     pad_frac=0.02\n# )\n# display(fig_facets)\n</code></pre>","tags":["remote-sensing","sentinel-2","time-series","fire","tipping-points","machine-learning","innovation-summit-2025"]},{"location":"remote_sensing/sentinel2_cloud_correction/","title":"Creating Cloud Corrected Sentinel-2 Images","text":"<p>Erick Verleye, ESIIL Software Developer</p> <p>A common issue when working with remote imaging data is that for a single image, it is likely that the instrument's field of view is at least partially occulted by clouds. A popular approach to producing a cloud-cleaned image over a particular region is to stitch  together a time series of images to create a composite. </p> <p>In this tutorial, we will create cloud-cleaned composites from Sentinel2 Level-1 C data acquired by the methods described at https://data-library.esiil.org/reomte_sensing/sentinel2_aws/sentinel2_aws. </p> <p>The steps to creating a cloud-cleaned composite are outlined as:  </p> <pre><code>- For each Sentinel-2 image in the time series:  \n    -- Use cloud file to filter image pixels which are 0 or above some threshold  \n- Combine filtered images over the entire time series into one composite using the median of pixel values\n</code></pre> <p>First let's look at the file structure created after downloading the data:  </p> <p></p> <p>There is information about the location, date, and file contents in the file name. The structure is . For example, the file 35_N_RA_2017_6_17_0_B02.jp2 covers the 35NRA military grid tile, is from the date 6-17-2017, and contains Band 2 data. The '0' between the band is the sequence, and can be ignored. More information on military grid tiles can be found at https://www.maptools.com/tutorials/mgrs/quick_guide <p>The strategy will be to process the files as groups according to their military grid tile and band across the entire time series. For example, a single processing group for tile 36NTF and Band 2 would be:  </p> <p>[ 2017_6_17/36_N_TF_2017_6_17_0_B02.jp2, 2017_6_24/36_N_TF_2017_6_24_0_B02.jp2, 2017_6_27/36_N_TF_2017_6_27_0_B02.jp2 ]</p> <p>The cloud file for each military tile and date will always end in '_qi_MSK_CLOUDS_B00.gml' and can be used to correct all other bands.  </p> <p>Let's now introduce some code that will allow us to process a single tile and band at a time.  </p> <p>In this tutorial we are creating composites from a relatively short time series. In practice, it  may take several months worth of data to create a sufficiently cloud-free composite. Working with  large time series sets is a memory intensive process. Thus, we will also be implementing a 'slice' parameter which allows the user to specify how many slices the images should be split into when processing.  For example, when the slices parameter is 20, only 1/20th of each image will be cloud corrected at a time.  At the end, these slices will be stitched back together.</p> <p>First, we must import some modules. The non-native package versions used are:  </p> <ul> <li>rasterio==1.3.6</li> <li>geopandas==0.13.0</li> <li>numpy==1.24.3</li> <li>tqdm==4.65.0</li> </ul> <p>You must also have GDAL installed, the version used for this tutorial is 3.6.4</p> <pre><code>import os \nimport warnings\nimport shutil\n\nimport rasterio\nfrom rasterio import features\nfrom rasterio.windows import Window\nimport geopandas as gpd\nimport numpy as np\nfrom tqdm import tqdm\n</code></pre> <p>Next, we define some constants and helper functions for loading in data and cleaning it:  </p> <pre><code>MAX_BAND_VAL = 4000  # counts\n\n# Helper function for opening Sentinel-2 jp2 files, with optional slicing\ndef get_img_from_file(img_path, g_ncols, dtype, row_bound=None):\n    img = rasterio.open(img_path, driver='JP2OpenJPEG')\n    ncols, nrows = img.meta['width'], img.meta['height']\n    assert g_ncols == ncols, f'Imgs have different size ncols: {ncols} neq {g_ncols}'\n    if row_bound is None:\n        pixels = img.read(1).astype(np.float32)\n    else:\n        pixels = img.read(\n            1,\n            window=Window.from_slices(\n                slice(row_bound[0], row_bound[1]),\n                slice(0, ncols)\n            )\n        ).astype(dtype)\n    return pixels\n\n# Helper function for reading in cloud file array, with optional slicing\ndef get_cloud_mask_from_file(cloud_path, crs, transform, shape, row_bound=None):\n    # filter out RuntimeWarnings, due to geopandas/fiona read file spam\n    # https://stackoverflow.com/questions/64995369/geopandas-warning-on-read-file\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    try:\n        cloud_file = gpd.read_file(cloud_path)\n        cloud_file.crs = (str(crs))\n        # convert the cloud mask data to a raster that has the same shape and transformation as the\n        # img raster data\n        cloud_img = features.rasterize(\n            (\n                (g['geometry'], 1) for v, g in cloud_file.iterrows()\n            ),\n            out_shape=shape,\n            transform=transform,\n            all_touched=True\n        )\n        if row_bound is None:\n            return np.where(cloud_img == 0, 1, 0)\n        return np.where(cloud_img[row_bound[0]:row_bound[1], :] == 0, 1, 0)\n    except Exception as e:\n        return None\n\n# Function for filtering out cloud pixels\ndef nan_clouds(pixels, cloud_channels, max_pixel_val: float = MAX_BAND_VAL):\n    cp = pixels * cloud_channels\n    mask = np.where(np.logical_or(cp == 0, cp &gt; max_pixel_val))\n    cp[mask] = np.nan\n    return cp\n</code></pre> <p>Finally we define our main function, which will create our cloud-cleaned composite for a single military  tile and optical band:  </p> <pre><code>def create_cloud_cleaned_composite(in_dir: str, mgrs_tile: str, band: str, out_file: str, num_slices: int = 12) -&gt; None:\n    \"\"\"\n    Creates a cloud cleaned composite tif file from a set of sentinel 2 files\n    Args:\n        in_dir (str): Directory containing all of the date directories which contain the sentinel 2 data\n        mgrs_tile (str): The military grid coordinate (35MGR, 36MTV, etc.) to create the composite for \n        band (str): The optical band for which to create the composite for (B02, B03, B08) etc.\n        num_slices (int): The amount of slices to split the composite up into while building it.\n                          More slices will use less RAM\n    \"\"\"\n    # Loop through each band, getting a median estimate for each\n    crs = None\n    transform = None\n\n    # Find each optical and cloud file in the input directory for the mgrs_tile and band combination\n    mgrs_str = f'{mgrs_tile[:2]}_{mgrs_tile[2]}_{mgrs_tile[3:]}'.upper()\n    band_str = band.upper()\n    dates = os.listdir(in_dir)\n    file_sets = []\n    for date_dir in dates:\n        optical_file = None\n        cloud_file = None\n        for file in os.listdir(os.path.join(in_dir, date_dir)):\n            f_up = file.upper()\n            file_path = os.path.join(in_dir, date_dir, file)\n            if mgrs_str in file and band_str in file:\n                optical_file = file_path\n            elif mgrs_str in file and 'MSK_CLOUDS_B00' in file:\n                cloud_file = file_path\n\n        if optical_file is None or cloud_file is None:\n            continue\n\n        file_sets.append((optical_file, cloud_file))\n\n    # Resolve the crs and other optical file attributes. Should be the same \n    # regardless of date so loop through all until found\n    crs = None\n    transform = None\n    g_nrows = None\n    g_ncols = None\n    for file_set in file_sets:\n        with rasterio.open(file_set[0], 'r', driver='JP2OpenJPEG') as rf:\n            g_nrows = rf.meta['height'] if g_nrows is None else g_nrows\n            g_ncols = rf.meta['width'] if g_ncols is None else g_ncols\n            crs = rf.crs if crs is None else crs\n            transform = rf.transform if transform is None else transform\n            if crs is not None and transform is not None and g_nrows is not None and g_ncols is not None:\n                break\n\n    if crs is None or transform is None or g_nrows is None or g_ncols is None:\n        raise LookupError(f'Could not determine the following projection attributes from the available '\n                          f'sentinel2 files in {in_dir}: \\n' \\\n                          f'{\"CRS\" if crs is None else \"\"} '\n                          f'{\"Transform\" if transform is None else \"\"} '\n                          f'{\"Number of rows\" if g_nrows is None else \"\"} '\n                          f'{\"Number of columns\" if g_ncols is None else \"\"}')\n\n    # Determine the slicing bounds to save memory as we process\n    slice_height = g_nrows / num_slices\n    slice_end_pts = [int(i) for i in np.arange(0, g_nrows + slice_height, slice_height)]\n    slice_bounds = [(slice_end_pts[i], slice_end_pts[i + 1] - 1) for i in range(num_slices - 1)]\n    slice_bounds.append((slice_end_pts[-2], slice_end_pts[-1]))   \n\n    # Correct the images one slice at a time, and then combine the slices. First create a temp directory in\n    # the out_dir to store the slices \n    slice_dir = os.path.join(os.path.dirname(out_file), 'slices')\n    os.makedirs(slice_dir, exist_ok=True)\n    for k, row_bound in tqdm(enumerate(slice_bounds), desc=f'band={band}', total=num_slices, position=2):\n        slice_file_path = os.path.join(slice_dir, f'{row_bound[0]}_{row_bound[1]}.tif')\n        cloud_correct_imgs = []\n        for file_set in tqdm(file_sets, desc=f'slice {k + 1}', leave=False, position=3):\n            # Get data from files\n            optical_file = file_set[0]\n            cloud_file = file_set[1]\n            pixels = get_img_from_file(optical_file, g_ncols, np.float32, row_bound)\n            cloud_channels = get_cloud_mask_from_file(cloud_file, crs, transform, (g_nrows, g_ncols), row_bound)\n            if cloud_channels is None:\n                continue\n            # add to list to do median filter later\n            cloud_correct_imgs.append(nan_clouds(pixels, cloud_channels))\n            del pixels\n        corrected_stack = np.vstack([img.ravel() for img in cloud_correct_imgs])\n        median_corrected = np.nanmedian(corrected_stack, axis=0, overwrite_input=True)\n        median_corrected = median_corrected.reshape(cloud_correct_imgs[0].shape)\n        with rasterio.open(slice_file_path, 'w', driver='GTiff', width=g_ncols, height=g_nrows,\n                           count=1, crs=crs, transform=transform, dtype=np.float32) as wf:\n            wf.write(median_corrected.astype(np.float32), 1)\n\n        # release mem\n        median_corrected = []\n        del median_corrected\n        corrected_stack = []\n        del corrected_stack\n\n    # Combine slices\n    with rasterio.open(out_file, 'w', driver='GTiff', width=g_ncols, height=g_nrows,\n                       count=1, crs=crs, transform=transform, dtype=np.float32) as wf:\n        for slice_file in os.listdir(slice_dir):\n            bound_split = slice_file.split('.')[0].split('_')\n            top_bound = int(bound_split[0])\n            bottom_bound = int(bound_split[1])\n            with rasterio.open(os.path.join(slice_dir, slice_file), 'r', driver='GTiff') as rf:\n                wf.write(\n                    rf.read(1),\n                    window=Window.from_slices(\n                        slice(top_bound, bottom_bound),\n                        slice(0, g_ncols)\n                    ),\n                    indexes=1\n                )\n\n    shutil.rmtree(slice_dir)\n    print(f'Wrote file to {out_file}')\n\n</code></pre> <p>Let's run an example now, using the files downloaded in the sentinel2 on aws tutorial for mgrs 36NUG and band B02. A figure of the file structure is included above. Make sure to change the in_dir and out_file parameters to match your environment.</p> <pre><code>create_cloud_cleaned_composite(in_dir=&lt;your_in_dir&gt;, mgrs_tile='36NUG', band='B02', out_file='test.tif')\n</code></pre> <p>This should create an output file called test.tif in your working directory that is cloud cleaned. Let's take a look at the results.  </p> <p>First, let's look at the sentinel2 B02 file used from each day:  </p> <p> </p> <p>As you can see there are many clouds, and sometimes the image is not fully populated with values.</p> <p>Let us now look at the output file, test.tif, and see if the clouds were removed and the images stitched together. </p> <p>We can see that all of the areas that used to be clouds are now NaN, or white in the image. As stated previously,  to get a full image with no NaNs may require months worth of data. It is achievable with this code, but too much for this tutuorial. Over several months there will be days where the areas now in white are not covered by clouds, and thus the median of those days' pixels will begin to fill in the white spots until you have a full image. </p>","tags":["remote-sensing","sentinel-2","cloud-correction"]},{"location":"time_series/prism_tipping_point_forecast/","title":"Finding Breaks and Forecasting Climate data using PRISM","text":"","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#introduction","title":"Introduction","text":"<p>Climate systems are often assumed to change gradually, but in many cases they exhibit abrupt shifts or \u201ctipping points\u201d where the rate of change accelerates or decelerates. Detecting and forecasting such breaks in climate time series is important for understanding regional climate impacts, anticipating ecosystem stress, and informing adaptation planning. This workflow combines data streaming, change-point detection, and time-series forecasting to provide a reproducible way of investigating potential tipping points in observed climate records.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#data-source-prism-climate-data","title":"Data Source: PRISM Climate Data","text":"<p>We use the Parameter-elevation Regressions on Independent Slopes Model (PRISM) dataset, developed by the PRISM Climate Group at Oregon State University. PRISM provides high-resolution gridded climate data for the contiguous United States, widely used in climate research and resource management (Daly et al. 2008, Int. J. Climatology).</p> <ul> <li>Coverage: 1981 to present (monthly time series).</li> <li>Variables: Mean, maximum, minimum temperature, and precipitation.</li> <li>Resolution: Typically 4 km, with finer options.</li> </ul> <p>By streaming PRISM directly via GDAL\u2019s Virtual File System (VSI), we avoid large downloads and enable point extraction (lat/lon or place-based geocoding) on demand.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Tipping points in climate: Shifts in warming rates or precipitation regimes can signify new stress regimes (Steffen et al. 2018, PNAS).</li> <li>Decision support: Early detection of accelerations in warming or drought is critical for water resource planning, agriculture, and fire risk management.</li> <li>Accessible forecasting: Many users lack the statistical background to implement advanced models. This workflow packages established methods into an easy-to-use pipeline.</li> </ol>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#step-1-data-streaming","title":"Step 1: Data Streaming","text":"<ul> <li>We extract PRISM monthly mean temperature (tmean) for a given location. </li> <li>Using GDAL, the script directly opens compressed <code>.zip</code> archives over HTTP without manual downloads. </li> <li>The code accounts for PRISM\u2019s use of scale/offset factors in raster bands, ensuring values are converted to real degrees Celsius.  </li> </ul>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#step-2-change-point-detection","title":"Step 2: Change-Point Detection","text":"<p>We ask: Has the climate trend changed significantly at some point? - Statistical foundation: Structural break detection in regression models (Bai &amp; Perron 2003, J. Econometrics).  - Algorithm: By default, we use the Pruned Exact Linear Time (PELT) method (Killick et al. 2012, JASA), implemented in the <code>ruptures</code> package (Truong et al. 2020, J. Statistical Software). - Fallback method: A custom F-test on segmented regressions compares slopes before and after each candidate break.</p> <p>Mathematics (simplified): For time series (y_t), we test piecewise linear models: [ y_t = \\alpha_1 + \\beta_1 t + \\varepsilon_t, \\quad t \\le k \\ y_t = \\alpha_2 + \\beta_2 t + \\varepsilon_t, \\quad t &gt; k ] We choose (k) that minimizes total residual error and test whether slopes differ significantly.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#step-3-forecasting-models","title":"Step 3: Forecasting Models","text":"<p>Forecasting is done with several standard models, then validated via rolling backtests: - Holt\u2013Winters Exponential Smoothing (Additive): captures trend + seasonality (Holt 1957; Winters 1960). - STL + ARIMA: Seasonal\u2013Trend decomposition with ARIMA errors (Box &amp; Jenkins 1970). - SARIMAX: Seasonal ARIMA with exogenous regressors, flexible for seasonal climate series. - Auto-ARIMA (pmdarima): Automated parameter selection (Hyndman &amp; Khandakar 2008).</p> <p>Forecast accuracy check: We use a rolling origin cross-validation (Tashman 2000, Int. J. Forecasting), where earlier subsets of data are used to predict withheld observations. The model with lowest RMSE (root mean square error) is chosen.</p> <p>Prediction intervals: Forecasts include 90% intervals, reflecting uncertainty in future outcomes.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#step-4-visualization","title":"Step 4: Visualization","text":"<p>The final plot shows: - Observed data (blue line). - Detected break (red dashed line). - Forecast mean (green line). - Prediction interval (light green band).</p> <p>This visualization helps both scientists and decision-makers quickly assess whether climate trends have shifted and what plausible futures look like.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#example-use-case","title":"Example Use Case","text":"<p>Suppose you want to know whether Boulder, Colorado has experienced an accelerated warming trend since 1981 and what might happen in the next 20 years: 1. Provide <code>place=\"Boulder, CO\"</code> and a start year. 2. The workflow fetches PRISM monthly mean temperature. 3. It detects whether the slope changed (e.g., around 2000s). 4. It produces a 20-year forecast with confidence bands.</p>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#references-verified-with-context","title":"References (verified, with context)","text":"<ul> <li> <p>PRISM data \u2014 Daly, C., Halbleib, M., Smith, J. I., et al. (2008). Physiographically sensitive mapping of temperature and precipitation across the conterminous United States. International Journal of Climatology, 28(15), 2031\u20132064. DOI / Wiley. Open PDF via PRISM group: link. Context: foundational description of PRISM dataset, which underlies all data streamed here.</p> </li> <li> <p>Multiple structural breaks \u2014 Bai, J., &amp; Perron, P. (2003). Computation and analysis of multiple structural change models. Journal of Applied Econometrics, 18(1), 1\u201322. Wiley abstract / JSTOR. Context: provides statistical foundation for detecting and estimating breakpoints in regression models.</p> </li> <li> <p>PELT algorithm \u2014 Killick, R., Fearnhead, P., &amp; Eckley, I. A. (2012). Optimal Detection of Changepoints with a Linear Computational Cost. Journal of the American Statistical Association, 107(500), 1590\u20131598. Publisher / preprint. Context: efficient algorithm for change-point detection, used in the ruptures package.</p> </li> <li> <p>ruptures package \u2014 Truong, C., Oudre, L., &amp; Vayatis, N. (2018/2020). ruptures: change point detection in Python. Journal of Statistical Software; also Selective review of offline change point detection methods, Signal Processing, 167, 107299. ruptures paper (arXiv) \u2022 review (Elsevier) \u2022 project docs. Context: the actual Python package used here for breakpoint detection.</p> </li> <li> <p>Rolling-origin evaluation \u2014 Tashman, L. J. (2000). Out-of-sample tests of forecasting accuracy: an analysis and review. International Journal of Forecasting, 16(4), 437\u2013450. ScienceDirect / RePEc. Context: justifies rolling-origin cross-validation used here to compare forecast models.</p> </li> <li> <p>Exponential smoothing (origin) \u2014 Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted moving averages. Office of Naval Research Report; reprinted International Journal of Forecasting (2004). IJF reprint abstract. Context: origin of exponential smoothing, one of our candidate forecasting models.</p> </li> <li> <p>Holt\u2013Winters seasonal method \u2014 Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6(3), 324\u2013342. JSTOR. Context: extends Holt\u2019s method with seasonality, used here in Holt\u2013Winters model.</p> </li> <li> <p>ARIMA framework \u2014 Box, G. E. P., &amp; Jenkins, G. M. (1970). Time Series Analysis: Forecasting and Control. Holden\u2011Day. (See later editions at Wiley; first edition archived: Internet Archive). Context: canonical reference for ARIMA/SARIMA models applied here.</p> </li> <li> <p>Auto-ARIMA \u2014 Hyndman, R. J., &amp; Khandakar, Y. (2008). Automatic time series forecasting: the forecast package for R. Journal of Statistical Software, 27(3), 1\u201322. JSS / PDF. Context: describes automated ARIMA search; foundation for pmdarima implementation in Python.</p> </li> <li> <p>Tipping points framing \u2014 Steffen, W., Rockstr\u00f6m, J., Richardson, K., et al. (2018). Trajectories of the Earth System in the Anthropocene. Proceedings of the National Academy of Sciences, 115(33), 8252\u20138259. PNAS / PDF. Context: frames the idea of global and regional climate tipping points, motivating the broader application of this workflow.</p> </li> </ul>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"time_series/prism_tipping_point_forecast/#code","title":"Code","text":"<p>Note: Runtime can be slow, especially for the first run at a new location. This is due to network fetches and model optimization. Patience pays off with a robust result!</p> <pre><code># PRISM tipping-point forecast (VSI + scale/offset safe + month backoff)\n# GDAL compatibility for InvGeoTransform return shape\nfrom __future__ import annotations\nimport warnings, datetime as dt\nfrom typing import Optional, Tuple, Dict, Union, List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import YearLocator, DateFormatter\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.forecasting.stl import STLForecast\nfrom statsmodels.tsa.arima.model import ARIMA\n\nfrom osgeo import gdal\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ---------- GDAL helpers ----------\ndef _inv_geotransform(gt: Tuple[float, ...]) -&gt; Optional[Tuple[float, ...]]:\n    \"\"\"\n    Return inverted geotransform as a 6-tuple, handling both GDAL APIs:\n      - old: returns (success_flag, inv_gt)\n      - new: returns inv_gt\n    \"\"\"\n    try:\n        out = gdal.InvGeoTransform(gt)\n        # new API: a 6-tuple\n        if isinstance(out, (list, tuple)) and len(out) == 6:\n            return tuple(out)\n        # old API: (success, inv_gt)\n        if isinstance(out, (list, tuple)) and len(out) == 2:\n            ok, inv = out\n            return tuple(inv) if ok and isinstance(inv, (list, tuple)) and len(inv) == 6 else None\n    except Exception:\n        return None\n    return None\n\n# ---------- PRISM helpers (VSI) ----------\n_RES_CODE = {\"4km\": \"25m\", \"800m\": \"30s\", \"400m\": \"15s\"}  # filename tokens\n\ndef _as_datecode(date: Union[str, dt.date, dt.datetime], freq: str) -&gt; Tuple[str, str]:\n    \"\"\"Return (datecode, yyyy) for PRISM naming given freq ('daily'|'monthly'|'annual').\"\"\"\n    if isinstance(date, (dt.datetime, dt.date)):\n        d = date if isinstance(date, dt.date) and not isinstance(date, dt.datetime) else date.date()\n    elif isinstance(date, str):\n        s = date.strip()\n        if freq == \"daily\":\n            try: d = dt.datetime.strptime(s, \"%Y-%m-%d\").date()\n            except ValueError: d = dt.datetime.strptime(s, \"%Y%m%d\").date()\n        elif freq == \"monthly\":\n            try: d = dt.datetime.strptime(s, \"%Y-%m\").date().replace(day=1)\n            except ValueError: d = dt.datetime.strptime(s, \"%Y%m\").date().replace(day=1)\n        elif freq == \"annual\":\n            d = dt.datetime.strptime(s, \"%Y\").date().replace(month=1, day=1)\n        else:\n            raise ValueError(\"freq must be one of: 'daily','monthly','annual'\")\n    else:\n        raise TypeError(\"date must be str, datetime, or date\")\n\n    if freq == \"daily\":\n        return d.strftime(\"%Y%m%d\"), d.strftime(\"%Y\")\n    elif freq == \"monthly\":\n        return d.strftime(\"%Y%m\"), d.strftime(\"%Y\")\n    else:\n        return d.strftime(\"%Y\"), d.strftime(\"%Y\")\n\ndef _build_prism_vsi(\n    variable: str,\n    date: Union[str, dt.date, dt.datetime],\n    resolution: str = \"4km\",\n    region: str = \"us\",\n    freq: str = \"monthly\",\n    network: str = \"an\",\n) -&gt; str:\n    if resolution not in _RES_CODE:\n        raise ValueError(\"resolution must be one of {'800m','4km','400m'}\")\n    datecode, yyyy = _as_datecode(date, freq)\n    res_code = _RES_CODE[resolution]\n    base_dir = f\"https://data.prism.oregonstate.edu/time_series/{region}/{network}/{resolution}/{variable}/{freq}/{yyyy}/\"\n    zip_name = f\"prism_{variable}_{region}_{res_code}_{datecode}.zip\"\n    tif_name = f\"prism_{variable}_{region}_{res_code}_{datecode}.tif\"\n    return f\"/vsizip//vsicurl/{base_dir}{zip_name}/{tif_name}\"\n\ndef _gdal_open_vsi(vsi: str) -&gt; gdal.Dataset | None:\n    # Make vsicurl streaming robust\n    gdal.SetConfigOption(\"GDAL_DISABLE_READDIR_ON_OPEN\", \"YES\")\n    gdal.SetConfigOption(\"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\", \".zip,.tif,.tiff,.xml,.stx,.prj,.aux.xml\")\n    gdal.SetConfigOption(\"GDAL_HTTP_MULTIRANGE\", \"YES\")\n    gdal.SetConfigOption(\"CPL_VSIL_CURL_CHUNK_SIZE\", \"1048576\")  # 1 MB\n    return gdal.Open(vsi, gdal.GA_ReadOnly)\n\ndef _safe_default_end_month(end: Optional[str]) -&gt; str:\n    \"\"\"Use previous full calendar month (YYYY-MM) if end is None.\"\"\"\n    if end is not None:\n        return end\n    today = pd.Timestamp.today().normalize()\n    return (today.replace(day=1) - pd.offsets.MonthBegin(1)).strftime(\"%Y-%m\")\n\ndef _read_point_scaled(ds: gdal.Dataset, lat: float, lon: float) -&gt; Optional[float]:\n    \"\"\"Read one pixel at (lat,lon), applying scale/offset; return \u00b0C or None.\"\"\"\n    gt = ds.GetGeoTransform()\n    inv_gt = _inv_geotransform(gt)\n    if inv_gt is None:\n        return None\n    px, py = gdal.ApplyGeoTransform(inv_gt, lon, lat)\n    pxi, pyi = int(round(px)), int(round(py))\n    if pxi &lt; 0 or pyi &lt; 0 or pxi &gt;= ds.RasterXSize or pyi &gt;= ds.RasterYSize:\n        return None\n    band = ds.GetRasterBand(1)\n    raw = float(band.ReadAsArray(pxi, pyi, 1, 1)[0, 0])\n    ndv = band.GetNoDataValue()\n    if ndv is not None and raw == ndv:\n        return None\n    scale = band.GetScale() if band.GetScale() is not None else 1.0\n    offset = band.GetOffset() if band.GetOffset() is not None else 0.0\n    val = raw * float(scale) + float(offset)\n    if not (-100.0 &lt;= val &lt;= 100.0):  # sanity AFTER scaling\n        return None\n    return val\n\ndef _sample_prism_month(lat: float, lon: float, when: dt.date) -&gt; Optional[float]:\n    \"\"\"\n    Return **scaled** tmean (\u00b0C) at (lat,lon) for the target month.\n    If that month's file isn't published yet, step back up to 6 months.\n    \"\"\"\n    for back in range(0, 7):\n        ts_try = (pd.Timestamp(when) - pd.offsets.MonthBegin(back)).to_period(\"M\").to_timestamp().date()\n        vsi = _build_prism_vsi(variable=\"tmean\", date=ts_try, resolution=\"4km\", region=\"us\", freq=\"monthly\", network=\"an\")\n        ds = _gdal_open_vsi(vsi)\n        if ds is None:\n            continue\n        try:\n            val = _read_point_scaled(ds, lat, lon)\n            if val is not None:\n                return val\n        finally:\n            ds = None\n    return None\n\ndef _stream_prism_monthly_tmean_vsi(lat: float, lon: float, start: Optional[str], end: Optional[str]) -&gt; pd.Series:\n    if start is None:\n        start = \"1981-01\"\n    end = _safe_default_end_month(end)\n\n    idx = pd.period_range(start=start, end=end, freq=\"M\").to_timestamp(how=\"start\")\n    vals, dates = [], []\n    for ts in idx:\n        val = _sample_prism_month(lat, lon, ts.date().replace(day=1))\n        if val is not None:\n            vals.append(val); dates.append(ts)\n\n    if not dates:\n        raise RuntimeError(\"No PRISM monthly samples could be read for the requested location/range.\")\n    s = pd.Series(vals, index=pd.to_datetime(dates)).sort_index().asfreq(\"MS\")\n    s = s.interpolate(limit=1)\n    s.name = \"tmean_c\"\n    return s\n\n# -------------------------\n# Optional geocoding\n# -------------------------\ndef _geocode_place(place: str) -&gt; Tuple[float, float]:\n    try:\n        from geopy.geocoders import Nominatim\n    except Exception as e:\n        raise ImportError(\"geopy is required to geocode place names. Try: pip install geopy\") from e\n    geolocator = Nominatim(user_agent=\"prism-forecast-geocoder\")\n    loc = geolocator.geocode(place)\n    if not loc:\n        raise ValueError(f\"Could not geocode place: {place}\")\n    return float(loc.latitude), float(loc.longitude)\n\n# -------------------------\n# Change-point detection (one break)\n# -------------------------\ndef _detect_breakpoint(y: pd.Series) -&gt; Optional[pd.Timestamp]:\n    try:\n        import ruptures as rpt\n        arr = y.values.reshape(-1, 1)\n        algo = rpt.Pelt(model=\"rbf\").fit(arr)\n        res = algo.predict(pen=8)\n        if len(res) &lt;= 1:\n            return None\n        idx = res[0] - 1\n        if 3 &lt;= idx &lt;= len(y) - 4:\n            return y.index[idx]\n        return None\n    except Exception:\n        window = max(12, len(y)//12)\n        if window*2 &gt;= len(y):\n            return None\n        x = np.arange(len(y))\n        def slope(a, b):\n            X = np.vstack([np.ones_like(a), a]).T\n            beta = np.linalg.lstsq(X, b, rcond=None)[0]\n            return beta[1]\n        best = None; best_jump = 0.0\n        for i in range(window, len(y)-window):\n            left = slope(x[i-window:i], y.values[i-window:i])\n            right = slope(x[i:i+window], y.values[i:i+window])\n            jump = abs(right - left)\n            if jump &gt; best_jump and 6 &lt; i &lt; len(y)-6:\n                best = i; best_jump = jump\n        return y.index[best] if best is not None else None\n\n# -------------------------\n# Rolling-origin backtest\n# -------------------------\ndef _rolling_rmse(y: pd.Series, model_fn, steps=12, min_train=120):\n    y = y.dropna()\n    errs = []\n    for end in range(min_train, len(y)-steps+1, steps):\n        train = y.iloc[:end]\n        test = y.iloc[end:end+steps]\n        try:\n            fc = model_fn(train, horizon=len(test))\n            fc = fc.reindex(test.index)\n            errs.append(((fc - test)**2).mean()**0.5)\n        except Exception:\n            continue\n    return float(np.nanmean(errs)) if errs else np.inf\n\n# -------------------------\n# Candidate model wrappers\n# -------------------------\ndef _fit_predict_es(train: pd.Series, horizon: int, return_pi=False):\n    mod = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12)\n    res = mod.fit(optimized=True, use_brute=True)\n    fc = res.forecast(horizon)\n    if return_pi:\n        resid = train - res.fittedvalues\n        std = float(np.nanstd(resid)); z = 1.64\n        lo = fc - z*std; hi = fc + z*std\n        return fc, lo, hi, {\"name\": \"Holt-Winters (add-add)\", \"aic\": np.nan}\n    return fc\n\ndef _fit_predict_stl_arima(train: pd.Series, horizon: int, return_pi=False):\n    stlf = STLForecast(endog=train, model=ARIMA, model_kwargs={\"order\": (1,1,1)}, period=12, robust=True)\n    res = stlf.fit()\n    fc = res.forecast(horizon)\n    if return_pi:\n        resid = (train - res.fittedvalues).dropna()\n        std = float(np.nanstd(resid)); z = 1.64\n        lo = fc - z*std; hi = fc + z*std\n        return fc, lo, hi, {\"name\": \"STL + ARIMA(1,1,1)\", \"aic\": np.nan}\n    return fc\n\ndef _fit_predict_sarimax(train: pd.Series, horizon: int, return_pi=False):\n    best_res = None; best = (np.inf, None, None)\n    orders = [(1,1,0), (1,1,1), (2,1,1)]\n    sorders = [(0,1,1,12), (1,1,1,12)]\n    for order in orders:\n        for sorder in sorders:\n            try:\n                mod = SARIMAX(train, order=order, seasonal_order=sorder, enforce_stationarity=False, enforce_invertibility=False)\n                res = mod.fit(disp=False)\n                if res.aic &lt; best[0]:\n                    best = (res.aic, order, sorder); best_res = res\n            except Exception:\n                continue\n    if best_res is None:\n        mod = SARIMAX(train, order=(1,1,1), seasonal_order=(0,1,1,12), enforce_stationarity=False, enforce_invertibility=False)\n        best_res = mod.fit(disp=False); label = \"SARIMAX fallback\"\n    else:\n        aic, order, sorder = best; label = f\"SARIMAX {order} x {sorder}\"\n    fc_res = best_res.get_forecast(steps=horizon)\n    fc = fc_res.predicted_mean\n    if return_pi:\n        conf = fc_res.conf_int(alpha=0.10)\n        lo = conf.iloc[:,0]; hi = conf.iloc[:,1]\n        return fc, lo, hi, {\"name\": label, \"aic\": float(best_res.aic)}\n    return fc\n\ndef _fit_predict_pmdarima(train: pd.Series, horizon: int, return_pi=False):\n    try:\n        import pmdarima as pm\n    except Exception:\n        if return_pi:\n            raise\n        return _fit_predict_sarimax(train, horizon, return_pi=return_pi)\n    model = pm.auto_arima(\n        train, seasonal=True, m=12, information_criterion=\"aic\", stepwise=True,\n        suppress_warnings=True, error_action=\"ignore\", max_p=3, max_q=3, max_P=2, max_Q=2, d=None, D=None\n    )\n    fc_idx = pd.date_range(train.index[-1] + pd.offsets.MonthBegin(1), periods=horizon, freq=\"MS\")\n    fc = pd.Series(model.predict(n_periods=horizon), index=fc_idx)\n    if return_pi:\n        vals, conf = model.predict(n_periods=horizon, return_conf_int=True, alpha=0.10)\n        lo = pd.Series(conf[:,0], index=fc_idx); hi = pd.Series(conf[:,1], index=fc_idx)\n        return fc, lo, hi, {\"name\": \"auto_arima (pmdarima)\", \"aic\": float(model.aic())}\n    return fc\n\n# -------------------------\n# Main function\n# -------------------------\ndef plot_prism_tipping_point_forecast(\n    place: Optional[str] = None,\n    lat: Optional[float] = None,\n    lon: Optional[float] = None,\n    start: Optional[str] = \"1981-01\",\n    end: Optional[str] = None,\n    max_forecast_years: int = 20,\n    figsize: Tuple[int, int] = (11, 6),\n    detect_break: bool = True,\n    prefer_pmdarima: bool = True,\n    seed: int = 1337,\n) -&gt; Tuple[plt.Figure, Dict, pd.Series]:\n    \"\"\"Stream PRISM monthly tmean (\u00b0C) via GDAL VSI, detect break, and forecast. Returns (fig, model_info, series).\"\"\"\n    np.random.seed(seed)\n\n    if place is None and (lat is None or lon is None):\n        raise ValueError(\"Provide either a place name OR both lat and lon.\")\n    if place is not None and (lat is None or lon is None):\n        lat, lon = _geocode_place(place)\n\n    y = _stream_prism_monthly_tmean_vsi(lat, lon, start, end)\n    if len(y) &lt; 60:\n        raise ValueError(\"Need at least ~5 years of monthly data for stable forecasting.\")\n\n    bp_ts = _detect_breakpoint(y) if detect_break else None\n\n    def es_fn(train, horizon): return _fit_predict_es(train, horizon)\n    def stl_fn(train, horizon): return _fit_predict_stl_arima(train, horizon)\n    def sarimax_fn(train, horizon): return _fit_predict_sarimax(train, horizon)\n\n    cands = [(\"Holt-Winters\", es_fn), (\"STL+ARIMA\", stl_fn), (\"SARIMAX\", sarimax_fn)]\n    if prefer_pmdarima:\n        try:\n            def pmd_fn(train, horizon): return _fit_predict_pmdarima(train, horizon)\n            cands.insert(0, (\"auto_arima\", pmd_fn))\n        except Exception:\n            pass\n\n    scores = []\n    for name, fn in cands:\n        rmse = _rolling_rmse(y, fn, steps=12, min_train=min(120, max(72, len(y)//2)))\n        scores.append((rmse, name, fn))\n    scores.sort(key=lambda x: x[0])\n    best_rmse, best_name, best_fn = scores[0]\n\n    horizon = int(max_forecast_years * 12)\n    if best_name == \"Holt-Winters\":\n        fc, lo, hi, info = _fit_predict_es(y, horizon, return_pi=True)\n    elif best_name == \"STL+ARIMA\":\n        fc, lo, hi, info = _fit_predict_stl_arima(y, horizon, return_pi=True)\n    elif best_name == \"SARIMAX\":\n        fc, lo, hi, info = _fit_predict_sarimax(y, horizon, return_pi=True)\n    else:\n        try:\n            fc, lo, hi, info = _fit_predict_pmdarima(y, horizon, return_pi=True)\n        except Exception:\n            fc, lo, hi, info = _fit_predict_sarimax(y, horizon, return_pi=True)\n            best_name = \"SARIMAX (fallback)\"\n\n    model_info = {\"winner\": best_name, \"rmse_validation\": best_rmse}\n    model_info.update(info)\n\n    # Plotting\n    C_OBS = \"#1f77b4\"; C_BREAK = \"#d62728\"; C_FC = \"#2ca02c\"; C_PI = \"#a6dba0\"\n    fig, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n    ax.plot(y.index, y.values, color=C_OBS, lw=1.8, label=\"Observed\")\n    if bp_ts is not None and bp_ts in y.index:\n        ax.axvline(bp_ts, color=C_BREAK, lw=1.5, ls=\"--\")\n        ax.text(bp_ts, ax.get_ylim()[1], \"  break\", color=C_BREAK, va=\"top\", ha=\"left\")\n    ax.fill_between(fc.index, lo.values, hi.values, color=C_PI, alpha=0.35, label=\"90% PI\")\n    ax.plot(fc.index, fc.values, color=C_FC, lw=2.0, label=\"Forecast\")\n\n    ax.set_ylabel(\"Monthly mean temperature (\u00b0C)\")\n    ax.set_xlabel(\"Year\")\n    ax.xaxis.set_major_locator(YearLocator(base=5))\n    ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n    ax.grid(True, axis=\"y\", alpha=0.15)\n\n    title_loc = place if place is not None else f\"{lat:.4f}, {lon:.4f}\"\n    start_str = y.index.min().strftime(\"%Y-%m\"); end_str = y.index.max().strftime(\"%Y-%m\")\n    ax.set_title(f\"PRISM tmean at {title_loc}\")\n    subtitle = f\"Data: {start_str} to {end_str}  \u2022  Break: {bp_ts.date() if bp_ts is not None else 'none'}  \u2022  Model: {model_info['winner']}\"\n    ax.text(0.01, 1.02, subtitle, transform=ax.transAxes, ha=\"left\", va=\"bottom\", fontsize=9, alpha=0.9)\n\n    ax.legend(frameon=False, ncol=3, loc=\"upper left\")\n    ax.margins(x=0.01)\n\n    plt.show(fig)\n    return fig, model_info, y\n\n# Example:\nfig, info, series = plot_prism_tipping_point_forecast(place=\"Boulder, CO\", start=\"1981-01\", max_forecast_years=20)\n</code></pre> <pre><code># PRISM tipping-point forecast in R (VSI + month backoff + scale/offset safe)\n# ---------------------------------------------------------------------------\n# Suggested mamba installs (terminal):\n#   mamba install -c conda-forge r-terra r-ggplot2 r-lubridate r-dplyr r-tidyr r-forecast r-scales\n# Optional:\n#   mamba install -c conda-forge r-strucchange        # breakpoints (preferred)\n#   mamba install -c conda-forge r-changepoint        # alternative break detector\n#   mamba install -c conda-forge r-tidygeocoder       # place=\"City, ST\" support\n\nsuppressPackageStartupMessages({\n  library(terra)\n  library(ggplot2)\n  library(lubridate)\n  library(dplyr)\n  library(tidyr)\n  library(forecast)\n  library(scales)\n})\n\n# ---------- PRISM helpers (VSI) ----------\n.res_code &lt;- c(\"4km\"=\"25m\",\"800m\"=\"30s\",\"400m\"=\"15s\")\n\n.as_datecode &lt;- function(date, freq=c(\"monthly\",\"daily\",\"annual\")){\n  freq &lt;- match.arg(freq)\n  if (inherits(date, c(\"Date\",\"POSIXt\"))) {\n    d &lt;- as.Date(date)\n  } else if (is.character(date)) {\n    s &lt;- trimws(date)\n    if (freq==\"daily\"){\n      if (grepl(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\", s)) d &lt;- as.Date(s)\n      else d &lt;- as.Date(s, format=\"%Y%m%d\")\n    } else if (freq==\"monthly\"){\n      if (grepl(\"^\\\\d{4}-\\\\d{2}$\", s)) d &lt;- as.Date(paste0(s,\"-01\"))\n      else d &lt;- as.Date(paste0(s,\"01\"), format=\"%Y%m%d\")\n    } else {\n      d &lt;- as.Date(paste0(s,\"-01-01\"))\n    }\n  } else stop(\"date must be character or Date/POSIXt\")\n\n  if (freq==\"daily\") list(datecode=format(d,\"%Y%m%d\"), yyyy=format(d,\"%Y\"))\n  else if (freq==\"monthly\") list(datecode=format(d,\"%Y%m\"), yyyy=format(d,\"%Y\"))\n  else list(datecode=format(d,\"%Y\"), yyyy=format(d,\"%Y\"))\n}\n\nbuild_prism_vsi &lt;- function(variable=\"tmean\",\n                            date=\"2025-06\",\n                            resolution=\"4km\",\n                            region=\"us\",\n                            freq=\"monthly\",\n                            network=\"an\"){\n  if (!resolution %in% names(.res_code)) stop(\"resolution must be one of {'800m','4km','400m'}\")\n  dc &lt;- .as_datecode(date, freq=freq)\n  res_token &lt;- unname(.res_code[[resolution]])\n  base_dir &lt;- sprintf(\"https://data.prism.oregonstate.edu/time_series/%s/%s/%s/%s/%s/%s/\",\n                      region, network, resolution, variable, freq, dc$yyyy)\n  zip_name &lt;- sprintf(\"prism_%s_%s_%s_%s.zip\", variable, region, res_token, dc$datecode)\n  tif_name &lt;- sprintf(\"prism_%s_%s_%s_%s.tif\",  variable, region, res_token, dc$datecode)\n  sprintf(\"/vsizip//vsicurl/%s%s/%s\", base_dir, zip_name, tif_name)\n}\n\nsafe_default_end_month &lt;- function(end=NULL){\n  if (!is.null(end)) return(end)\n  today &lt;- floor_date(Sys.Date(), \"month\")\n  format(today - months(1), \"%Y-%m\")\n}\n\n# Scale/offset helper:\napply_scale_sanity &lt;- function(x){\n  if (all(is.na(x))) return(NA_real_)\n  v &lt;- as.numeric(x)[1]\n  if (is.na(v)) return(NA_real_)\n  # If values look like degC*10, normalize\n  if (abs(v) &gt; 120) v &lt;- v / 10.0\n  # sanity bound\n  if (v &lt; -100 || v &gt; 100) return(NA_real_)\n  v\n}\n\nsample_prism_month &lt;- function(lat, lon, when, max_back=6L){\n  # Try target month, then step back up to max_back months\n  for (b in 0:max_back){\n    ts_try &lt;- as.Date(floor_date(as.Date(when), \"month\") - months(b))\n    vsi &lt;- build_prism_vsi(variable=\"tmean\", date=ts_try, resolution=\"4km\",\n                           region=\"us\", freq=\"monthly\", network=\"an\")\n    r &lt;- tryCatch(terra::rast(vsi), error=function(e) NULL)\n    if (is.null(r)) next\n    val &lt;- tryCatch({\n      extract(r, matrix(c(lon, lat), ncol=2))[,2, drop=TRUE]\n    }, error=function(e) NA_real_)\n    val &lt;- apply_scale_sanity(val)\n    if (!is.na(val)) return(val)\n  }\n  return(NA_real_)\n}\n\nfill_single_gaps &lt;- function(x){\n  # Only fill isolated NA flanked by numbers; leave longer runs as NA\n  if (length(x) &lt; 3) return(x)\n  y &lt;- x\n  for (i in 2:(length(x)-1)){\n    if (is.na(x[i]) &amp;&amp; !is.na(x[i-1]) &amp;&amp; !is.na(x[i+1])){\n      y[i] &lt;- 0.5*(x[i-1]+x[i+1])\n    }\n  }\n  y\n}\n\nstream_prism_monthly_tmean &lt;- function(lat, lon, start=\"1981-01\", end=NULL){\n  end &lt;- safe_default_end_month(end)\n  idx &lt;- seq(from=as.Date(paste0(start,\"-01\")),\n             to=as.Date(paste0(end,\"-01\")),\n             by=\"1 month\")\n  vals &lt;- vapply(idx, function(d) sample_prism_month(lat, lon, d), numeric(1))\n  keep &lt;- !is.na(vals)\n  if (!any(keep)) stop(\"No PRISM monthly samples could be read for the requested location/range.\")\n  df &lt;- tibble::tibble(date=idx, tmean_c=vals)\n  df$tmean_c &lt;- fill_single_gaps(df$tmean_c)\n  df %&gt;% filter(!is.na(tmean_c)) %&gt;% arrange(date)\n}\n\n# ---------- Optional geocoding ----------\n# If tidygeocoder is installed, use it; else try a tiny built-in lookup; else stop.\n.geocode_builtin &lt;- tibble::tibble(\n  place = c(\"Boulder, CO\",\"Denver, CO\",\"Phoenix, AZ\",\"Seattle, WA\",\"New York, NY\",\"Los Angeles, CA\"),\n  lat   = c(40.0150, 39.7392, 33.4484, 47.6062, 40.7128, 34.0522),\n  lon   = c(-105.2705,-104.9903,-112.0740,-122.3321,-74.0060,-118.2437)\n)\n\ngeocode_place &lt;- function(place){\n  if (requireNamespace(\"tidygeocoder\", quietly=TRUE)){\n    res &lt;- tidygeocoder::geocode(address=place, method=\"osm\", quiet=TRUE)\n    if (nrow(res)&gt;0 &amp;&amp; !is.na(res$lat[1]) &amp;&amp; !is.na(res$long[1])) {\n      return(c(lat=res$lat[1], lon=res$long[1]))\n    }\n  }\n  hit &lt;- .geocode_builtin %&gt;% filter(tolower(place)==tolower(.data$place))\n  if (nrow(hit)==1) return(c(lat=hit$lat, lon=hit$lon))\n  stop(\"Geocoding failed; install 'tidygeocoder' (or add to built-in lookup) or pass lat/lon.\")\n}\n\n# ---------- Change-point detection (one break) ----------\n# Strategy: try strucchange -&gt; changepoint -&gt; OLS fallback\n\ndetect_breakpoint &lt;- function(df_monthly){\n  y &lt;- df_monthly$tmean_c\n  t &lt;- seq_along(y)\n\n  # 1) strucchange (slope break in linear trend)\n  if (requireNamespace(\"strucchange\", quietly=TRUE)){\n    dat &lt;- data.frame(y=y, t=t)\n    bp &lt;- tryCatch(strucchange::breakpoints(y ~ t, data=dat, h=max(12, floor(length(y)/12))),\n                   error=function(e) NULL)\n    k &lt;- if (!is.null(bp) &amp;&amp; length(bp$breakpoints)&gt;0) bp$breakpoints[1] else NA_integer_\n    if (!is.na(k) &amp;&amp; k &gt;= 6 &amp;&amp; k &lt;= (length(y)-6)) return(df_monthly$date[k])\n  }\n\n  # 2) changepoint (mean/var break)\n  if (requireNamespace(\"changepoint\", quietly=TRUE)){\n    cp &lt;- tryCatch(changepoint::cpt.meanvar(y, method=\"PELT\", penalty=\"SIC\"),\n                   error=function(e) NULL)\n    k &lt;- if (!is.null(cp) &amp;&amp; length(changepoint::cpts(cp))&gt;0) changepoint::cpts(cp)[1] else NA_integer_\n    if (!is.na(k) &amp;&amp; k &gt;= 6 &amp;&amp; k &lt;= (length(y)-6)) return(df_monthly$date[k])\n  }\n\n  # 3) OLS segmented fallback: pick k minimizing RSS with two lines\n  ols_line &lt;- function(x, y){\n    X &lt;- cbind(1, x)\n    coef &lt;- tryCatch(solve(t(X)%*%X, t(X)%*%y), error=function(e) c(NA,NA))\n    yhat &lt;- as.vector(X%*%coef)\n    rss &lt;- sum((y-yhat)^2)\n    list(a=coef[1], b=coef[2], rss=rss)\n  }\n  n &lt;- length(y)\n  window &lt;- max(12, floor(n/12))\n  best_k &lt;- NA; best_rss &lt;- Inf\n  for (k in seq(window, n-window)){\n    left  &lt;- ols_line(t[1:k], y[1:k])$rss\n    right &lt;- ols_line(t[(k+1):n], y[(k+1):n])$rss\n    tot &lt;- left + right\n    if (!is.na(tot) &amp;&amp; tot &lt; best_rss){ best_rss &lt;- tot; best_k &lt;- k }\n  }\n  if (is.na(best_k) || best_k &lt; 6 || best_k &gt; (n-6)) return(NULL)\n  df_monthly$date[best_k]\n}\n\n# ---------- Rolling-origin backtest ----------\nrolling_rmse &lt;- function(ts_y, fit_fn, steps=12, min_train=120){\n  n &lt;- length(ts_y)\n  errs &lt;- c()\n  for (end in seq(min_train, n-steps, by=steps)){\n    train &lt;- ts_y[1:end]\n    test  &lt;- ts_y[(end+1):(end+steps)]\n    fc &lt;- tryCatch(fit_fn(train, horizon=length(test)), error=function(e) rep(NA_real_, length(test)))\n    if (length(fc) == length(test) &amp;&amp; all(!is.na(fc))) {\n      errs &lt;- c(errs, sqrt(mean((fc - test)^2)))\n    }\n  }\n  if (length(errs)==0) Inf else mean(errs)\n}\n\n# ---------- Candidate model wrappers ----------\nfit_predict_ets &lt;- function(train, horizon, return_pi=FALSE){\n  m &lt;- forecast::ets(train, model=\"AAA\")\n  fc &lt;- forecast::forecast(m, h=horizon, level=90)\n  if (return_pi) list(mean=as.numeric(fc$mean), lo=as.numeric(fc$lower[,1]), hi=as.numeric(fc$upper[,1]),\n                      label=\"ETS(A,A,A)\", aic=AIC(m))\n  else as.numeric(fc$mean)\n}\n\nfit_predict_stl_arima &lt;- function(train, horizon, return_pi=FALSE){\n  m &lt;- forecast::stlm(train, method=\"arima\")\n  fc &lt;- forecast::forecast(m, h=horizon, level=90)\n  if (return_pi) list(mean=as.numeric(fc$mean), lo=as.numeric(fc$lower[,1]), hi=as.numeric(fc$upper[,1]),\n                      label=\"STL + ARIMA\", aic=NA_real_)\n  else as.numeric(fc$mean)\n}\n\nfit_predict_auto_arima &lt;- function(train, horizon, return_pi=FALSE){\n  m &lt;- forecast::auto.arima(train, seasonal=TRUE, stepwise=TRUE, approximation=FALSE)\n  fc &lt;- forecast::forecast(m, h=horizon, level=90)\n  if (return_pi) list(mean=as.numeric(fc$mean), lo=as.numeric(fc$lower[,1]), hi=as.numeric(fc$upper[,1]),\n                      label=sprintf(\"auto.arima %s\", paste(forecast::arimaorder(m), collapse=\",\")),\n                      aic=AIC(m))\n  else as.numeric(fc$mean)\n}\n\n# ---------- Main function ----------\nplot_prism_tipping_point_forecast &lt;- function(place=NULL,\n                                              lat=NULL, lon=NULL,\n                                              start=\"1981-01\",\n                                              end=NULL,\n                                              max_forecast_years=20,\n                                              detect_break=TRUE,\n                                              seed=1337){\n  set.seed(seed)\n  if (is.null(place) &amp;&amp; (is.null(lat) || is.null(lon)))\n    stop(\"Provide either a place name OR both lat and lon.\")\n  if (!is.null(place) &amp;&amp; (is.null(lat) || is.null(lon))){\n    coords &lt;- tryCatch(geocode_place(place), error=function(e) NULL)\n    if (is.null(coords)) stop(\"Geocoding failed; install 'tidygeocoder' (or add to built-in lookup) or pass lat/lon.\")\n    lat &lt;- coords[\"lat\"]; lon &lt;- coords[\"lon\"]\n  }\n\n  df &lt;- stream_prism_monthly_tmean(lat, lon, start=start, end=end)\n  if (nrow(df) &lt; 60) stop(\"Need at least ~5 years of monthly data for stable forecasting.\")\n\n  # Break detection\n  bp_date &lt;- if (detect_break) detect_breakpoint(df) else NULL\n\n  # Monthly ts (freq=12)\n  y_ts &lt;- ts(df$tmean_c, start=c(year(min(df$date)), month(min(df$date))), frequency=12)\n\n  # Candidates\n  fitA &lt;- function(z, horizon) fit_predict_auto_arima(z, horizon)\n  fitB &lt;- function(z, horizon) fit_predict_stl_arima(z, horizon)\n  fitC &lt;- function(z, horizon) fit_predict_ets(z, horizon)\n\n  base_min_train &lt;- min(120, max(72, floor(length(y_ts)/2)))\n  rmseA &lt;- rolling_rmse(y_ts, fitA, steps=12, min_train=base_min_train)\n  rmseB &lt;- rolling_rmse(y_ts, fitB, steps=12, min_train=base_min_train)\n  rmseC &lt;- rolling_rmse(y_ts, fitC, steps=12, min_train=base_min_train)\n\n  scores &lt;- tibble::tibble(model=c(\"auto.arima\",\"STL+ARIMA\",\"ETS\"),\n                           rmse=c(rmseA, rmseB, rmseC))\n  winner &lt;- scores$model[which.min(scores$rmse)]\n\n  horizon &lt;- max_forecast_years * 12\n  if (winner==\"ETS\"){\n    res &lt;- fit_predict_ets(y_ts, horizon, return_pi=TRUE)\n  } else if (winner==\"STL+ARIMA\"){\n    res &lt;- fit_predict_stl_arima(y_ts, horizon, return_pi=TRUE)\n  } else {\n    res &lt;- fit_predict_auto_arima(y_ts, horizon, return_pi=TRUE)\n  }\n\n  # Build forecast df with dates\n  last_month &lt;- max(df$date)\n  fidx &lt;- seq(from=last_month %m+% months(1), by=\"1 month\", length.out=horizon)\n  fc_df &lt;- tibble::tibble(date=fidx, mean=res$mean, lo=res$lo, hi=res$hi)\n\n  # Plot\n  C_OBS &lt;- \"#1f77b4\"; C_BREAK &lt;- \"#d62728\"; C_FC &lt;- \"#2ca02c\"; C_PI &lt;- \"#a6dba0\"\n\n  p &lt;- ggplot() +\n    geom_line(data=df, aes(date, tmean_c), color=C_OBS, linewidth=0.7) +\n    { if (!is.null(bp_date)) geom_vline(xintercept=as.numeric(bp_date), color=C_BREAK, linetype=\"dashed\") } +\n    geom_ribbon(data=fc_df, aes(date, ymin=lo, ymax=hi), fill=C_PI, alpha=0.35) +\n    geom_line(data=fc_df, aes(date, mean), color=C_FC, linewidth=0.8) +\n    scale_x_date(date_breaks = \"5 years\", date_labels = \"%Y\", expand=expansion(mult=c(0.01,0.02))) +\n    labs(x=\"Year\", y=\"Monthly mean temperature (\u00b0C)\",\n         title = sprintf(\"PRISM tmean at %s\",\n                         if (!is.null(place)) place else sprintf(\"%.4f, %.4f\", lat, lon)),\n         subtitle = sprintf(\"Data: %s to %s  \u2022  Break: %s  \u2022  Model: %s  \u2022  RMSE (val): %s\",\n                            format(min(df$date), \"%Y-%m\"),\n                            format(max(df$date), \"%Y-%m\"),\n                            if (!is.null(bp_date)) format(bp_date, \"%Y-%m\") else \"none\",\n                            winner, scales::comma(min(scores$rmse)))) +\n    theme_minimal(base_size = 11) +\n    theme(panel.grid.minor = element_blank())\n\n  print(p)\n\n  list(plot=p,\n       model_info=list(winner=winner, rmse_validation=min(scores$rmse), label=winner),\n       series=df)\n}\n\n# ----------------------------\n# Examples:\n# ----------------------------\n\n# 1) With coordinates (no extra installs):\nout &lt;- plot_prism_tipping_point_forecast(\n   lat = 40.015, lon = -105.2705,   # Boulder, CO\n   start = \"1981-01\",\n   max_forecast_years = 20\n )\n\n# 2) With place name (install tidygeocoder first, or rely on built-in few-city lookup):\n# out &lt;- plot_prism_tipping_point_forecast(\n#   place = \"Boulder, CO\",\n#   start = \"1981-01\",\n#   max_forecast_years = 20\n# )\n</code></pre>","tags":["time-series","forecasting","tipping-points","climate","innovation-summit-2025"]},{"location":"topic/forecasting/","title":"forecasting","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> </ul>"},{"location":"topic/time-series/","title":"time-series","text":"<ul> <li>Finding Breaks and Forecasting Climate data using PRISM</li> <li>Burning Boundaries: Random Forest Early Warnings for Post-Fire Collapse</li> </ul>"}]}